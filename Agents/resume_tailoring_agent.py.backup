import os
import time
import uuid
import re
import json
import unicodedata
import requests
from google import genai
import dotenv
from typing import List, Tuple, Optional, Dict, Any
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError


dotenv.load_dotenv()

# --- Mimikree API Configuration ---
MIMIKREE_BASE_URL = os.getenv("MIMIKREE_BASE_URL", "http://localhost:3000")

# --- Google API Setup ---
SCOPES = [
    "https://www.googleapis.com/auth/documents",
    "https://www.googleapis.com/auth/drive",
]


# ============================================================================
# QUALITY VALIDATION & OPTIMIZATION SYSTEM
# ============================================================================

class LineQualityValidator:
    """Validates and fixes line quality issues (orphaned words, short lines, etc.)"""
    
    def __init__(self):
        self.min_words = 5  # Minimum words per line (except headers)
        self.min_char_utilization = 0.5  # Minimum 50% character utilization
        self.awkward_endings = ['and', 'or', 'with', 'for', 'the', 'a', 'an', 'of', ',']
    
    def validate_line(self, line_text: str, line_meta: dict) -> dict:
        """
        Validate a single line's quality.
        
        Returns:
            {
                'is_valid': bool,
                'issues': list of issue dicts,
                'quality_score': float (0-100)
            }
        """
        issues = []
        is_header = line_meta.get('is_header', False)
        
        # Skip validation for empty lines and headers
        if not line_text.strip() or is_header:
            return {'is_valid': True, 'issues': [], 'quality_score': 100.0}
        
        words = line_text.split()
        char_count = len(line_text)
        max_chars = line_meta.get('char_limit', 100)
        
        # Issue 1: Orphaned words (single word on line)
        if len(words) == 1:
            issues.append({
                'type': 'orphaned_word',
                'severity': 'critical',
                'message': f"Single word '{words[0]}' on its own line",
                'suggestion': 'MUST merge with previous or next line'
            })
        
        # Issue 2: Too few words
        elif len(words) < self.min_words:
            issues.append({
                'type': 'insufficient_words',
                'severity': 'high',
                'message': f'Only {len(words)} words (min: {self.min_words})',
                'suggestion': 'Expand content or merge with adjacent line'
            })
        
        # Issue 3: Character underutilization
        if max_chars > 0:
            utilization = char_count / max_chars
            if utilization < self.min_char_utilization:
                issues.append({
                    'type': 'underutilized',
                    'severity': 'medium',
                    'message': f'Only {utilization:.1%} of available space used',
                    'suggestion': f'Can add ~{int(max_chars * 0.7 - char_count)} more characters'
                })
        
        # Issue 4: Awkward line endings
        last_word = words[-1].rstrip('.,;:!?').lower()
        if last_word in self.awkward_endings:
            issues.append({
                'type': 'awkward_ending',
                'severity': 'medium',
                'message': f"Line ends awkwardly with '{words[-1]}'",
                'suggestion': 'Reword to end at natural break point'
            })
        
        # Calculate quality score
        critical_count = sum(1 for i in issues if i['severity'] == 'critical')
        high_count = sum(1 for i in issues if i['severity'] == 'high')
        medium_count = sum(1 for i in issues if i['severity'] == 'medium')
        
        quality_score = 100 - (critical_count * 40) - (high_count * 20) - (medium_count * 10)
        quality_score = max(0, quality_score)
        
        is_valid = critical_count == 0
        
        return {
            'is_valid': is_valid,
            'issues': issues,
            'quality_score': quality_score,
            'word_count': len(words),
            'char_utilization': utilization if max_chars > 0 else 1.0
        }
    
    def validate_document_lines(self, line_metadata: list) -> dict:
        """
        Validate all lines in document.
        
        Returns comprehensive quality report.
        """
        results = []
        total_issues = []
        
        for idx, line_meta in enumerate(line_metadata):
            line_text = line_meta.get('text', '')
            validation = self.validate_line(line_text, line_meta)
            
            results.append({
                'line_number': idx + 1,
                'text_preview': line_text[:60] + '...' if len(line_text) > 60 else line_text,
                'validation': validation
            })
            
            # Collect all issues
            for issue in validation['issues']:
                total_issues.append({
                    'line_number': idx + 1,
                    'line_text': line_text[:60],
                    **issue
                })
        
        # Calculate overall scores
        valid_lines = sum(1 for r in results if r['validation']['is_valid'])
        avg_quality = sum(r['validation']['quality_score'] for r in results) / len(results) if results else 0
        
        critical_issues = [i for i in total_issues if i['severity'] == 'critical']
        high_issues = [i for i in total_issues if i['severity'] == 'high']
        
        return {
            'total_lines': len(results),
            'valid_lines': valid_lines,
            'invalid_lines': len(results) - valid_lines,
            'average_quality_score': avg_quality,
            'total_issues': len(total_issues),
            'critical_issues': len(critical_issues),
            'high_issues': len(high_issues),
            'issues_by_line': results,
            'all_issues': total_issues,
            'pass': len(critical_issues) == 0
        }


class ContentQualityScorer:
    """Scores bullet points and content quality"""
    
    def __init__(self):
        # Action verb tiers
        self.weak_verbs = ['helped', 'worked on', 'responsible for', 'did', 'made', 'was', 'had']
        self.medium_verbs = ['created', 'built', 'developed', 'implemented', 'designed', 'managed']
        self.strong_verbs = ['architected', 'spearheaded', 'pioneered', 'orchestrated', 'engineered', 'established']
        self.quantified_verbs = ['increased', 'reduced', 'accelerated', 'optimized', 'improved', 'enhanced', 'boosted']
    
    def score_bullet_point(self, bullet: str, job_keywords: list = None) -> dict:
        """
        Score a bullet point on multiple dimensions.
        
        Returns scores (0-100) for:
        - Action verb strength
        - Quantification
        - Keyword relevance
        - Impact clarity
        - Specificity
        - Overall score
        """
        scores = {}
        
        # 1. Action Verb Strength (0-100)
        first_word = bullet.split()[0].lower().rstrip('.,;:!?') if bullet.split() else ''
        
        if first_word in self.weak_verbs:
            scores['action_verb'] = 25
            scores['action_verb_feedback'] = f"Weak verb '{first_word}' - use stronger alternatives"
        elif first_word in self.medium_verbs:
            scores['action_verb'] = 60
            scores['action_verb_feedback'] = f"Medium verb '{first_word}' - consider stronger alternatives"
        elif first_word in self.strong_verbs:
            scores['action_verb'] = 85
            scores['action_verb_feedback'] = f"Strong verb '{first_word}' ‚úì"
        elif first_word in self.quantified_verbs:
            scores['action_verb'] = 100
            scores['action_verb_feedback'] = f"Excellent quantified verb '{first_word}' ‚úì"
        else:
            scores['action_verb'] = 40
            scores['action_verb_feedback'] = f"Unknown verb '{first_word}' - verify strength"
        
        # 2. Quantification Score (0-100)
        numbers = re.findall(r'\d+[\d,\.]*[%xXkKmMbB+]?', bullet)
        if len(numbers) >= 2:
            scores['quantification'] = 100
            scores['quantification_feedback'] = f"Excellent - {len(numbers)} metrics found ‚úì"
        elif len(numbers) == 1:
            scores['quantification'] = 70
            scores['quantification_feedback'] = f"Good - 1 metric found, consider adding more"
        else:
            scores['quantification'] = 0
            scores['quantification_feedback'] = "CRITICAL - No quantification! Add metrics/percentages"
        
        # 3. Keyword Relevance (0-100)
        if job_keywords:
            keyword_matches = sum(1 for kw in job_keywords if kw.lower() in bullet.lower())
            scores['relevance'] = min(100, keyword_matches * 30)
            scores['relevance_feedback'] = f"{keyword_matches} job keywords found"
        else:
            scores['relevance'] = 50
            scores['relevance_feedback'] = "No job keywords provided for comparison"
        
        # 4. Impact Clarity - STAR format (0-100)
        has_context = any(word in bullet.lower() for word in [' for ', ' in ', ' across ', ' to '])
        has_action = any(word in bullet.lower() for word in [' by ', ' through ', ' via ', ' using '])
        has_result = len(numbers) > 0 or any(word in bullet.lower() for word in ['resulting', 'leading to', 'achieving'])
        
        star_score = (has_context * 30) + (has_action * 30) + (has_result * 40)
        scores['impact_clarity'] = star_score
        scores['impact_clarity_feedback'] = f"STAR elements: Context:{has_context}, Action:{has_action}, Result:{has_result}"
        
        # 5. Specificity (0-100)
        vague_words = ['various', 'multiple', 'several', 'many', 'some', 'numerous']
        has_vague = any(word in bullet.lower() for word in vague_words)
        proper_nouns = len(re.findall(r'\b[A-Z][a-z]+\b', bullet))  # Specific technologies, tools
        
        if has_vague:
            scores['specificity'] = 30
            scores['specificity_feedback'] = "Contains vague words - be more specific"
        elif proper_nouns >= 3:
            scores['specificity'] = 90
            scores['specificity_feedback'] = f"Very specific - {proper_nouns} specific terms ‚úì"
        elif proper_nouns >= 1:
            scores['specificity'] = 70
            scores['specificity_feedback'] = f"Good specificity - {proper_nouns} specific terms"
        else:
            scores['specificity'] = 50
            scores['specificity_feedback'] = "Could be more specific - add tools/technologies"
        
        # Overall score
        score_values = [scores['action_verb'], scores['quantification'], 
                       scores.get('relevance', 50), scores['impact_clarity'], scores['specificity']]
        overall = sum(score_values) / len(score_values)
        
        # Grade
        if overall >= 90:
            grade = 'A (Excellent)'
        elif overall >= 80:
            grade = 'B (Good)'
        elif overall >= 70:
            grade = 'C (Acceptable)'
        elif overall >= 60:
            grade = 'D (Needs Improvement)'
        else:
            grade = 'F (Poor - Must Fix)'
        
        return {
            'overall_score': overall,
            'grade': grade,
            'scores': scores,
            'needs_improvement': overall < 80
        }


class ATSOptimizer:
    """Optimizes resume for Applicant Tracking Systems"""
    
    def __init__(self):
        self.standard_headers = {
            'experience': ['EXPERIENCE', 'WORK EXPERIENCE', 'PROFESSIONAL EXPERIENCE'],
            'education': ['EDUCATION', 'ACADEMIC BACKGROUND'],
            'skills': ['SKILLS', 'TECHNICAL SKILLS', 'CORE COMPETENCIES'],
            'projects': ['PROJECTS', 'KEY PROJECTS', 'NOTABLE PROJECTS']
        }
    
    def analyze_ats_compatibility(self, resume_text: str, job_keywords: list) -> dict:
        """
        Analyze ATS compatibility.

        Returns ATS score and recommendations.
        """
        issues = []
        score = 100

        # Debug logging
        print(f"\nüîç ATS DEBUG: Analyzing {len(job_keywords)} keywords")
        if job_keywords:
            print(f"   First 5 keywords: {job_keywords[:5]}")

        # 1. Keyword Density Analysis
        keyword_counts = {}
        resume_lower = resume_text.lower()

        for keyword in job_keywords:
            keyword_lower = keyword.lower()

            # For multi-word keywords, use simple substring matching
            # For single-word keywords, use word boundary matching
            if ' ' in keyword_lower or '-' in keyword_lower:
                # Multi-word or hyphenated: use substring matching
                count = resume_lower.count(keyword_lower)
            else:
                # Single word: use word boundary matching for accuracy
                count = len(re.findall(r'\b' + re.escape(keyword_lower) + r'\b', resume_lower))

            keyword_counts[keyword] = count

            if count == 0:
                issues.append({
                    'type': 'missing_keyword',
                    'severity': 'critical',
                    'keyword': keyword,
                    'message': f"Required keyword '{keyword}' not found",
                    'recommendation': 'Add to relevant sections'
                })
                score -= 15
                print(f"   ‚ùå Missing: '{keyword}'")
            elif count == 1:
                issues.append({
                    'type': 'low_keyword_density',
                    'severity': 'medium',
                    'keyword': keyword,
                    'message': f"Keyword '{keyword}' appears only once",
                    'recommendation': 'Use 2-3 times for better ATS ranking'
                })
                score -= 5
                print(f"   ‚ö†Ô∏è  Low density (1x): '{keyword}'")
            else:
                print(f"   ‚úì Good ({count}x): '{keyword}'")
        
        # 2. Keyword Placement (should appear early)
        first_third = len(resume_text) // 3
        early_content = resume_text[:first_third].lower()
        
        critical_keywords_early = sum(
            1 for kw in job_keywords[:min(5, len(job_keywords))]
            if kw.lower() in early_content
        )
        
        if critical_keywords_early < 3:
            issues.append({
                'type': 'keyword_placement',
                'severity': 'high',
                'message': 'Critical keywords should appear in first 1/3 of resume',
                'recommendation': 'Reorder content to place key skills/experience early'
            })
            score -= 10
        
        # Calculate final score
        score = max(0, score)
        
        return {
            'ats_score': score,
            'grade': self._score_to_grade(score),
            'keyword_counts': keyword_counts,
            'issues': issues,
            'pass': score >= 80,
            'recommendations': [issue['recommendation'] for issue in issues]
        }
    
    def _score_to_grade(self, score: float) -> str:
        if score >= 90:
            return 'A (Excellent ATS)'
        elif score >= 80:
            return 'B (Good ATS)'
        elif score >= 70:
            return 'C (Acceptable ATS)'
        else:
            return 'F (Poor ATS - Will Likely Be Filtered)'


class MultiStageValidator:
    """Comprehensive multi-stage validation"""
    
    def __init__(self):
        self.line_validator = LineQualityValidator()
        self.content_scorer = ContentQualityScorer()
        self.ats_optimizer = ATSOptimizer()
    
    def validate_resume(self, resume_text: str, line_metadata: list, job_keywords: list) -> dict:
        """
        Run complete validation suite.
        
        Returns comprehensive validation report.
        """
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'validations': {}
        }
        
        print("\n" + "="*60)
        print("üîç COMPREHENSIVE QUALITY VALIDATION")
        print("="*60)
        
        # Stage 1: Line Quality
        print("\nüìè Stage 1: Line Quality Validation...")
        line_validation = self.line_validator.validate_document_lines(line_metadata)
        report['validations']['line_quality'] = line_validation
        print(f"   Score: {line_validation['average_quality_score']:.1f}/100")
        print(f"   Issues: {line_validation['total_issues']} ({line_validation['critical_issues']} critical)")
        
        # Stage 2: Content Quality
        print("\nüìù Stage 2: Content Quality Analysis...")
        bullet_points = self._extract_bullet_points(resume_text)
        content_scores = []
        poor_bullets = []
        
        for bullet in bullet_points:
            score_result = self.content_scorer.score_bullet_point(bullet, job_keywords)
            content_scores.append(score_result['overall_score'])
            if score_result['overall_score'] < 80:
                poor_bullets.append({
                    'text': bullet[:70] + '...' if len(bullet) > 70 else bullet,
                    'score': score_result['overall_score'],
                    'grade': score_result['grade'],
                    'issues': score_result['scores']
                })
        
        avg_content_score = sum(content_scores) / len(content_scores) if content_scores else 0
        report['validations']['content_quality'] = {
            'score': avg_content_score,
            'total_bullets': len(bullet_points),
            'poor_bullets_count': len(poor_bullets),
            'poor_bullets': poor_bullets
        }
        print(f"   Score: {avg_content_score:.1f}/100")
        print(f"   Bullets analyzed: {len(bullet_points)}")
        print(f"   Needs improvement: {len(poor_bullets)}")
        
        # Stage 3: ATS Optimization
        print("\nü§ñ Stage 3: ATS Compatibility...")
        ats_validation = self.ats_optimizer.analyze_ats_compatibility(resume_text, job_keywords)
        report['validations']['ats'] = ats_validation
        print(f"   Score: {ats_validation['ats_score']:.1f}/100")
        print(f"   Issues: {len(ats_validation['issues'])}")
        
        # Calculate Overall Score
        scores = {
            'line_quality': line_validation['average_quality_score'],
            'content_quality': avg_content_score,
            'ats': ats_validation['ats_score']
        }
        overall_score = sum(scores.values()) / len(scores)
        
        report['overall_score'] = overall_score
        report['scores'] = scores
        report['grade'] = self._score_to_grade(overall_score)
        
        # Determine if resume is ready
        report['is_ready'] = (
            line_validation['pass'] and
            avg_content_score >= 80 and
            ats_validation['pass'] and
            overall_score >= 85
        )
        
        print(f"\n{'='*60}")
        print(f"üéØ OVERALL SCORE: {overall_score:.1f}/100 ({report['grade']})")
        print(f"{'='*60}")
        print(f"‚úì Line Quality: {scores['line_quality']:.1f}/100")
        print(f"‚úì Content Quality: {scores['content_quality']:.1f}/100")
        print(f"‚úì ATS Compatibility: {scores['ats']:.1f}/100")
        print(f"\n{'‚úÖ READY TO SUBMIT' if report['is_ready'] else '‚ö†Ô∏è NEEDS IMPROVEMENT'}")
        
        return report
    
    def _extract_bullet_points(self, text: str) -> list:
        """Extract bullet points from resume text"""
        # Match common bullet patterns
        bullets = re.findall(r'[‚Ä¢\-\*]\s*([^\n‚Ä¢\-\*]+)', text)
        return [b.strip() for b in bullets if b.strip() and len(b.strip()) > 10]
    
    def _score_to_grade(self, score: float) -> str:
        if score >= 95:
            return 'A+ (Exceptional)'
        elif score >= 90:
            return 'A (Excellent)'
        elif score >= 85:
            return 'B+ (Very Good)'
        elif score >= 80:
            return 'B (Good)'
        elif score >= 75:
            return 'C+ (Acceptable)'
        elif score >= 70:
            return 'C (Needs Work)'
        else:
            return 'F (Poor - Must Improve)'


# ============================================================================
# END QUALITY VALIDATION SYSTEM
# ============================================================================


def get_google_services(credentials=None):
    """Get Google Docs and Drive services.

    Args:
        credentials: Optional Google OAuth2 Credentials object. If not provided,
                    falls back to token.json file.

    Returns:
        Tuple of (docs_service, drive_service)
    """
    creds = credentials

    # If no credentials provided, use legacy token.json file
    if not creds:
        # The file token.json stores the user's access and refresh tokens, and is
        # created automatically when the authorization flow completes for the first
        # time.
        if os.path.exists("token.json"):
            creds = Credentials.from_authorized_user_file("token.json", SCOPES)
        # If there are no (valid) credentials available, let the user log in.
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                flow = InstalledAppFlow.from_client_secrets_file(
                    "../credentials.json", SCOPES
                )
                creds = flow.run_local_server(port=0)
            # Save the credentials for the next run
            with open("token.json", "w") as token:
                token.write(creds.to_json())

    try:
        docs_service = build("docs", "v1", credentials=creds)
        drive_service = build("drive", "v3", credentials=creds)
        return docs_service, drive_service
    except HttpError as err:
        print(err)
        return None, None

def get_doc_id_from_url(url):
    """Extracts the Google Doc ID from a URL."""
    match = re.search(r"/document/d/([a-zA-Z0-9-_]+)", url)
    if match:
        return match.group(1)
    return None


# --- Mimikree API Integration ---

def authenticate_mimikree(email: str, password: str) -> Optional[Dict[str, Any]]:
    """Authenticate with Mimikree API and get username.

    Args:
        email: Mimikree account email
        password: Mimikree account password

    Returns:
        Dictionary with:
        - success: Boolean
        - username: Username if successful
        - name: Full name if successful
        - message: Error message if failed
    """
    try:
        response = requests.post(
            f"{MIMIKREE_BASE_URL}/api/external/authenticate",
            headers={"Content-Type": "application/json"},
            json={"email": email, "password": password},
            timeout=10
        )

        data = response.json()

        if response.status_code == 200 and data.get('success'):
            print(f"‚úÖ Mimikree authentication successful for user: {data.get('username')}")
            return data
        else:
            print(f"‚ùå Mimikree authentication failed: {data.get('message', 'Unknown error')}")
            return None

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Mimikree API connection error: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Unexpected error during Mimikree authentication: {e}")
        return None


def generate_mimikree_questions(job_description: str, job_title: str, company: str) -> List[str]:
    """Generate relevant questions to ask Mimikree chatbot based on job description.

    Args:
        job_description: Job description text
        job_title: Job title
        company: Company name

    Returns:
        List of questions to ask the Mimikree chatbot (max 20)
    """
    try:
        client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

        # Get max questions from environment or use default of 8 for faster processing
        max_questions = int(os.getenv("MIMIKREE_MAX_QUESTIONS", "8"))
        max_questions = max(5, min(max_questions, 20))  # Between 5 and 20

        prompt = f"""You are helping tailor a resume for a specific job position. Based on the job description below, generate a list of questions to ask a personal AI chatbot (Mimikree) that has access to the user's complete profile, projects, skills, and experience.

These questions should help gather specific information that would be valuable for tailoring the resume to this job.

CRITICAL RULES:
1. Generate EXACTLY {max_questions} high-quality questions (no more, no less)
2. Questions should be specific to the job requirements
3. Ask about relevant skills, projects, and experiences
4. Ask about specific technologies, tools, or methodologies mentioned in the job
5. Ask about quantifiable achievements related to the job requirements
6. Questions should be answerable by someone's personal AI assistant that knows their background

Job Title: {job_title}
Company: {company}

Job Description:
{job_description}

Return your response as a JSON array of strings:
{{
    "questions": [
        "Question 1",
        "Question 2",
        ...
    ]
}}

IMPORTANT - ASK ABOUT ALTERNATIVES:
- Ask about projects not currently on resume that match this job
- Ask about skills relevant to this job that may not be highlighted
- Ask about specific achievements with quantifiable results
- Ask about technologies, tools, and methodologies mentioned in the job description

EXAMPLES OF GOOD QUESTIONS:
- "What experience do you have with [specific technology from job]? Include projects and quantifiable results."
- "Do you have any projects involving [key skill from job] that aren't on your current resume?"
- "Have you worked with [tool/framework from job]? Describe your most impressive accomplishment with it."
- "What's your most relevant project for a [job title] role? Include metrics and impact."
- "Have you conducted [specific methodology from job]? Describe your approach and results."
- "What quantifiable achievements do you have related to [key responsibility from job]?"
- "Do you have experience with [domain/industry from job]? What projects demonstrate this?"
"""

        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )

        text = response.candidates[0].content.parts[0].text.strip()
        if text.startswith('```'):
            text = text.replace('```json', '').replace('```', '').strip()

        data = json.loads(text)
        questions = data.get('questions', [])

        # Limit to requested max
        questions = questions[:max_questions]

        print(f"‚úÖ Generated {len(questions)} questions for Mimikree chatbot")
        if len(questions) < max_questions:
            print(f"‚ö†Ô∏è  Note: Requested {max_questions} but only got {len(questions)} questions")
        return questions

    except Exception as e:
        print(f"‚ö†Ô∏è Error generating Mimikree questions: {e}")
        # Return some default questions as fallback
        return [
            f"What relevant experience do you have for a {job_title} position?",
            "What are your strongest technical skills?",
            "What quantifiable achievements do you have in your previous roles?",
            "What projects have you built that demonstrate your abilities?",
            "What technologies and tools are you most proficient with?"
        ]


def ask_mimikree_batch_questions(username: str, questions: List[str], max_retries: int = 2) -> Optional[Dict[str, Any]]:
    """Ask Mimikree chatbot multiple questions and get responses.

    Args:
        username: Mimikree username (from authentication)
        questions: List of questions to ask (max 20)
        max_retries: Maximum number of retry attempts (default 2)

    Returns:
        Dictionary with:
        - success: Boolean
        - username: Username
        - totalQuestions: Total questions asked
        - successfulResponses: Number of successful responses
        - responses: List of question-answer pairs
    """
    try:
        if not questions:
            print("‚ö†Ô∏è No questions provided to Mimikree API")
            return None

        # Limit to 20 questions (API maximum)
        questions = questions[:20]

        # Get timeout from environment or use default
        # Allow longer timeout for batch processing - calculating based on number of questions
        # Estimate: ~10-15 seconds per question for LLM processing
        base_timeout = 120  # 2 minutes base
        timeout_per_question = 10  # 10 seconds per question
        timeout = base_timeout + (len(questions) * timeout_per_question)
        timeout = min(timeout, 600)  # Cap at 10 minutes max

        print(f"‚è±Ô∏è  Estimated processing time: {timeout} seconds for {len(questions)} questions")

        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"üîÑ Retry attempt {attempt + 1}/{max_retries}...")

                response = requests.post(
                    f"{MIMIKREE_BASE_URL}/api/external/batch-questions",
                    headers={"Content-Type": "application/json"},
                    json={"username": username, "questions": questions},
                    timeout=timeout
                )

                data = response.json()

                if response.status_code == 200 and data.get('success'):
                    print(f"‚úÖ Mimikree batch questions successful: {data.get('successfulResponses')}/{data.get('totalQuestions')} responses")
                    return data
                else:
                    print(f"‚ùå Mimikree batch questions failed: {data.get('message', 'Unknown error')}")
                    if attempt < max_retries - 1:
                        print(f"‚è≥ Waiting before retry...")
                        time.sleep(5)  # Wait 5 seconds before retry
                    continue

            except requests.exceptions.Timeout as e:
                print(f"‚è±Ô∏è  Request timed out after {timeout} seconds")
                if attempt < max_retries - 1:
                    print(f"üí° Tip: Reduce number of questions or check if Mimikree server is running")
                    print(f"‚è≥ Waiting before retry...")
                    time.sleep(5)
                    continue
                else:
                    print(f"‚ùå All retry attempts exhausted")
                    return None

            except requests.exceptions.RequestException as e:
                print(f"‚ùå Mimikree API connection error: {e}")
                if attempt < max_retries - 1:
                    print(f"‚è≥ Waiting before retry...")
                    time.sleep(5)
                    continue
                else:
                    return None

        return None

    except Exception as e:
        print(f"‚ùå Unexpected error during Mimikree batch questions: {e}")
        return None


def format_mimikree_responses_for_resume(responses_data: Dict[str, Any]) -> str:
    """Format Mimikree responses into a structured text for resume tailoring.

    Args:
        responses_data: Response data from Mimikree batch questions API

    Returns:
        Formatted text containing all Q&A pairs
    """
    if not responses_data or not responses_data.get('success'):
        return ""

    formatted_text = "=== USER PROFILE INFORMATION FROM MIMIKREE ===\n\n"
    formatted_text += f"Based on questions about the user's background, here are the relevant details:\n\n"

    responses = responses_data.get('responses', [])
    successful_count = 0

    for idx, response in enumerate(responses, 1):
        if response.get('success'):
            question = response.get('question', '')
            answer = response.get('answer', '')
            formatted_text += f"Q{idx}: {question}\n"
            formatted_text += f"A{idx}: {answer}\n\n"
            successful_count += 1

    formatted_text += f"=== END OF USER PROFILE ({successful_count} relevant details) ===\n"

    return formatted_text

def copy_google_doc(drive_service, doc_id, new_title):
    """Creates a copy of the specified Google Doc."""
    try:
        copy_metadata = {'name': new_title}
        copied_file = drive_service.files().copy(fileId=doc_id, body=copy_metadata).execute()
        return copied_file.get('id')
    except HttpError as error:
        print(f"An error occurred while copying the document: {error}")
        return None

def read_google_doc_content(docs_service, document_id):
    """Reads and returns the text content of a Google Doc."""
    try:
        document = docs_service.documents().get(documentId=document_id).execute()
        content = document.get('body').get('content')
        return read_structural_elements(content)
    except HttpError as error:
        print(f"An error occurred while reading the document: {error}")
        return None

def read_structural_elements(elements):
    """Recursively reads text from Google Docs structural elements with styling info."""
    text = ''
    for value in elements:
        if 'paragraph' in value:
            para_elements = value.get('paragraph').get('elements')
            for elem in para_elements:
                if 'textRun' in elem:
                    text_run = elem.get('textRun', {})
                    content = text_run.get('content', '')
                    text_style = text_run.get('textStyle', {})
                    
                    # Check for URL link
                    link = text_style.get('link', {})
                    url = link.get('url', None) if link else None
                    
                    # Check for bold and italic styling
                    is_bold = text_style.get('bold', False)
                    is_italic = text_style.get('italic', False)
                    
                    # Wrap content with appropriate tags
                    # URLs get special treatment to preserve them
                    if url:
                        content = f'<url={url}>{content}</url>'
                    
                    if is_bold and is_italic:
                        content = f'<b><i>{content}</i></b>'
                    elif is_bold:
                        content = f'<b>{content}</b>'
                    elif is_italic:
                        content = f'<i>{content}</i>'
                    
                    text += content
    return text

def read_structural_elements_plain(elements):
    """Reads plain text from Google Docs structural elements without styling tags."""
    text = ''
    for value in elements:
        if 'paragraph' in value:
            para_elements = value.get('paragraph').get('elements')
            for elem in para_elements:
                text += elem.get('textRun', {}).get('content', '')
    return text

def detect_project_sections(resume_text: str, line_metadata: List[dict]) -> List[Dict[str, Any]]:
    """Detect project sections in resume (title + associated bullets).

    Args:
        resume_text: Plain text resume content
        line_metadata: Line metadata from extract_document_structure

    Returns:
        List of project dictionaries with:
        - title: Project title text
        - bullets: List of bullet point texts
        - start_line: Starting line number
        - end_line: Ending line number
        - full_text: Complete project text (title + all bullets)
    """
    projects = []
    lines = resume_text.split('\n')

    # Look for PROJECTS section
    in_projects_section = False
    current_project = None

    for i, line in enumerate(lines):
        line_stripped = line.strip()

        # Detect PROJECTS header
        if line_stripped.upper() in ['PROJECTS', 'PROJECT', 'PERSONAL PROJECTS', 'KEY PROJECTS']:
            in_projects_section = True
            continue

        # Exit projects section when we hit another header
        if in_projects_section and line_stripped.upper() in ['SKILLS', 'EDUCATION', 'EXPERIENCE', 'CERTIFICATIONS', 'ACHIEVEMENTS']:
            in_projects_section = False
            if current_project:
                projects.append(current_project)
                current_project = None
            continue

        if not in_projects_section:
            continue

        # Skip empty lines
        if not line_stripped:
            continue

        # Find matching metadata
        matching_meta = None
        for meta in line_metadata:
            if line_stripped in meta['text'] or meta['text'] in line_stripped:
                matching_meta = meta
                break

        # Detect project title (usually bold, no bullet, or starts with bold text)
        is_bullet = matching_meta and matching_meta.get('bullet_level', 0) > 0
        has_bold = '<b>' in line or (matching_meta and '<b>' in matching_meta.get('text', ''))

        if not is_bullet and (has_bold or len(line_stripped) < 100):
            # This looks like a project title
            if current_project:
                # Save previous project
                projects.append(current_project)

            current_project = {
                'title': line_stripped,
                'bullets': [],
                'start_line': i,
                'end_line': i,
                'lines': [line_stripped]
            }
        elif current_project and is_bullet:
            # This is a bullet point for the current project
            current_project['bullets'].append(line_stripped)
            current_project['end_line'] = i
            current_project['lines'].append(line_stripped)

    # Don't forget the last project
    if current_project:
        projects.append(current_project)

    # Add full_text for each project
    for project in projects:
        project['full_text'] = '\n'.join(project['lines'])

    return projects


def extract_document_structure(docs_service, document_id):
    """Extract line-by-line structure with formatting metadata, detecting natural line wraps.

    Returns a list of line metadata dictionaries containing:
    - text: the actual text content (may span multiple visual lines)
    - alignment: left, center, right, justified
    - bullet_level: nesting level (0 = no bullet, 1+ = nested)
    - indent_start: indentation in points
    - indent_first_line: first line indent in points
    - char_limit: estimated max characters before line wrap
    - line_number: sequential line number
    - visual_lines: number of lines this text spans on page
    """
    try:
        document = docs_service.documents().get(documentId=document_id).execute()
        content = document.get('body', {}).get('content', [])

        # Default page width (8.5" with 1" margins = 6.5" = 468 points)
        # Average character width ~7 points for typical fonts at 11pt
        default_page_width = 468
        avg_char_width = 7

        line_metadata = []
        line_number = 0

        for element in content:
            if 'paragraph' in element:
                paragraph = element['paragraph']
                para_style = paragraph.get('paragraphStyle', {})
                bullet = paragraph.get('bullet', None)

                # Get alignment (default: START/left)
                alignment_map = {
                    'START': 'left',
                    'CENTER': 'center',
                    'END': 'right',
                    'JUSTIFIED': 'justified'
                }
                alignment = alignment_map.get(para_style.get('alignment', 'START'), 'left')

                # Get indentation
                indent_start = para_style.get('indentStart', {}).get('magnitude', 0)
                indent_first_line = para_style.get('indentFirstLine', {}).get('magnitude', 0)

                # Determine bullet level
                bullet_level = 0
                if bullet:
                    # Bullet level is determined by nesting level
                    nesting_level = bullet.get('nestingLevel', 0)
                    bullet_level = nesting_level + 1
                    # Add extra indent for bullets (typically 36 points per level)
                    indent_start += bullet_level * 36

                # Extract text content from paragraph
                para_elements = paragraph.get('elements', [])
                line_text = ''
                for elem in para_elements:
                    if 'textRun' in elem:
                        line_text += elem.get('textRun', {}).get('content', '')

                # Remove trailing newlines for analysis
                line_text_stripped = line_text.rstrip('\n')

                # Skip empty lines (but keep them in structure)
                if not line_text_stripped:
                    continue

                # Calculate available width for FIRST line (with first line indent)
                first_line_width = default_page_width - indent_start - indent_first_line
                first_line_char_limit = max(0, int(first_line_width / avg_char_width))

                # Calculate available width for CONTINUATION lines (without first line indent)
                continuation_width = default_page_width - indent_start
                continuation_char_limit = max(0, int(continuation_width / avg_char_width))

                # Estimate how many visual lines this paragraph spans
                current_length = len(line_text_stripped)
                visual_lines = 1  # At least 1 line

                if current_length > first_line_char_limit:
                    # Text wraps beyond first line
                    remaining_chars = current_length - first_line_char_limit
                    additional_lines = (remaining_chars + continuation_char_limit - 1) // continuation_char_limit
                    visual_lines += additional_lines

                # For char_buffer calculation, use the last line's remaining space
                if visual_lines == 1:
                    char_buffer = max(0, first_line_char_limit - current_length)
                else:
                    # Calculate chars on the last line
                    chars_before_last_line = first_line_char_limit + (visual_lines - 2) * continuation_char_limit
                    chars_on_last_line = current_length - chars_before_last_line
                    char_buffer = max(0, continuation_char_limit - chars_on_last_line)

                line_metadata.append({
                    'text': line_text_stripped,
                    'alignment': alignment,
                    'bullet_level': bullet_level,
                    'indent_start': indent_start,
                    'indent_first_line': indent_first_line,
                    'char_limit_first_line': first_line_char_limit,
                    'char_limit_continuation': continuation_char_limit,
                    'current_length': current_length,
                    'char_buffer': char_buffer,
                    'visual_lines': visual_lines,
                    'line_number': line_number
                })

                line_number += 1

        return line_metadata

    except HttpError as error:
        print(f"Error extracting document structure: {error}")
        return []

def calculate_line_budget(line_metadata, max_additional_lines=0):
    """Calculate how many lines can be added to the resume.

    Args:
        line_metadata: List of line metadata from extract_document_structure
        max_additional_lines: Maximum number of lines that can be added (default 0 = strict page preservation)

    Returns:
        Dictionary with:
        - current_paragraphs: number of paragraphs
        - current_visual_lines: total visual lines (accounting for wrapping)
        - max_allowed_lines: maximum lines allowed
        - available_lines: how many more lines can be added
        - underutilized_lines: potential lines that could be freed by shortening
    """
    current_paragraphs = len(line_metadata)

    # Calculate TOTAL visual lines (accounting for text wrapping)
    current_visual_lines = sum(line.get('visual_lines', 1) for line in line_metadata)

    # Calculate potential lines that could be freed by shortening content
    underutilized_lines = sum(
        1 for line in line_metadata
        if line.get('char_buffer', 0) > 20  # More than 20 chars of buffer
    )

    return {
        'current_paragraphs': current_paragraphs,
        'current_visual_lines': current_visual_lines,
        'max_allowed_lines': current_visual_lines + max_additional_lines,
        'available_lines': max_additional_lines,
        'underutilized_lines': underutilized_lines
    }

def get_actual_page_count(drive_service, document_id):
    """Get ACTUAL page count by exporting to PDF and checking PDF pages.
    
    This is the ONLY reliable way to get true page count with formatting considered.
    
    Args:
        drive_service: Google Drive API service
        document_id: Document ID to check
        
    Returns:
        int: Actual number of pages in the document
    """
    try:
        import io
        from PyPDF2 import PdfReader
        
        # Export document as PDF
        request = drive_service.files().export_media(
            fileId=document_id,
            mimeType='application/pdf'
        )
        
        # Download PDF to memory
        pdf_bytes = io.BytesIO()
        pdf_bytes.write(request.execute())
        pdf_bytes.seek(0)
        
        # Read PDF and count pages
        pdf_reader = PdfReader(pdf_bytes)
        page_count = len(pdf_reader.pages)
        
        return page_count
    except ImportError:
        print("‚ö†Ô∏è  PyPDF2 not installed. Run: pip install PyPDF2")
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not get PDF page count: {e}")
        return None

def verify_document_length(docs_service, drive_service, document_id, original_page_count, tolerance=0):
    """Verify that document hasn't exceeded original page length.
    
    Args:
        docs_service: Google Docs API service
        drive_service: Google Drive API service
        document_id: Document ID to check
        original_page_count: Original number of pages (from PDF export)
        tolerance: Number of additional pages allowed (default 0 = strict)
        
    Returns:
        Dictionary with:
        - within_limit: True if document is within page limits
        - current_pages: Actual current page count
        - original_pages: Original page count
        - overflow_pages: Number of pages over limit
    """
    try:
        # Get ACTUAL page count from PDF export
        current_pages = get_actual_page_count(drive_service, document_id)
        
        if current_pages is None:
            # Fallback to paragraph count if PDF method fails
            print("‚ö†Ô∏è  Falling back to paragraph estimation (less accurate)")
            current_metadata = extract_document_structure(docs_service, document_id)
            actual_paragraphs = len(current_metadata)
            # Rough estimate: 40 paragraphs ‚âà 1 page
            current_pages = actual_paragraphs / 40.0
        
        max_allowed_pages = original_page_count + tolerance
        within_limit = current_pages <= max_allowed_pages
        overflow_pages = max(0, current_pages - max_allowed_pages)
        
        return {
            'within_limit': within_limit,
            'current_pages': current_pages,
            'original_pages': original_page_count,
            'max_allowed_pages': max_allowed_pages,
            'overflow_pages': overflow_pages
        }
    except Exception as e:
        print(f"Warning: Could not verify document length: {e}")
        # In case of error, assume it's okay to be conservative
        return {
            'within_limit': True,
            'current_visual_lines': original_visual_lines,
            'original_visual_lines': original_visual_lines,
            'max_allowed_lines': original_visual_lines,
            'overflow_lines': 0
        }

def score_replacement(replacement, line_metadata):
    """Score a replacement by its impact vs risk.
    
    Higher scores = better replacements (high impact, low risk of overflow)
    
    Returns:
        Score (float): Higher is better. Returns -1 if replacement is risky.
    """
    original_text = replacement.get('original_text', '')
    updated_text = replacement.get('updated_text', '')
    
    # Calculate length change
    original_len = len(original_text)
    updated_len = len(updated_text)
    length_change = updated_len - original_len
    
    # Find matching metadata for char_buffer
    char_buffer = None
    for meta in line_metadata or []:
        normalized_meta = _normalize_for_match(meta['text'])
        normalized_original = _normalize_for_match(original_text)
        if normalized_original in normalized_meta or normalized_meta in normalized_original:
            char_buffer = meta.get('char_buffer', 0)
            break
    
    # If we can't find metadata, be very conservative
    if char_buffer is None:
        if length_change > 0:
            return -1  # Reject additions when we don't know the buffer
        char_buffer = 0
    
    # Risk assessment
    if length_change > char_buffer:
        return -1  # Exceeds buffer - reject
    
    # Calculate impact score
    # Factors:
    # 1. Length-neutral or reductions are best (higher score)
    # 2. Small additions with large buffer are safe
    # 3. Longer original text = higher impact (more visible change)
    
    if length_change <= 0:
        # Length-neutral or reduction - excellent
        impact_score = 100 - length_change  # Reductions get bonus
    else:
        # Addition - score based on safety margin
        buffer_usage = length_change / max(char_buffer, 1)
        if buffer_usage > 0.6:  # Using >60% of buffer
            return -1  # Too risky
        impact_score = 50 * (1 - buffer_usage)  # Less buffer usage = higher score
    
    # Bonus for longer original text (more visible)
    visibility_bonus = min(20, original_len / 5)
    
    # Bonus if it has quantified data (important to preserve)
    if _extracts_quantified_data(original_text):
        visibility_bonus += 10
    
    return impact_score + visibility_bonus

def extract_job_keywords(job_description):
    """Extract important keywords and themes from job description using AI.

    Returns:
        Dictionary with:
        - required_skills: must-have technical/functional skills
        - preferred_skills: nice-to-have skills
        - key_themes: main themes (e.g., "leadership", "collaboration")
        - prioritized_keywords: ranked list of keywords to emphasize
    """
    try:
        client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

        prompt = f"""Analyze this job description and extract the most important keywords and themes.
Focus on skills, qualifications, and attributes that the employer is seeking.

Your task:
1. Identify REQUIRED skills (must-have technical and functional skills)
2. Identify PREFERRED skills (nice-to-have skills mentioned)
3. Extract KEY THEMES (e.g., "leadership", "innovation", "collaboration", "data-driven")
4. Create a PRIORITIZED list of keywords that should appear in the resume (most important first)

Return response in this JSON format:
{{
    "required_skills": ["skill1", "skill2", ...],
    "preferred_skills": ["skill1", "skill2", ...],
    "key_themes": ["theme1", "theme2", ...],
    "prioritized_keywords": ["keyword1", "keyword2", ...]
}}

JOB DESCRIPTION:
{job_description}
"""

        # Make API call
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )

        text = response.candidates[0].content.parts[0].text.strip()
        if text.startswith('```'):
            text = text.replace('```json', '').replace('```', '').strip()

        return json.loads(text)
    except Exception as e:
        print(f"Warning: Keyword extraction failed ({e}), continuing without keywords")
        return {
            "required_skills": [],
            "preferred_skills": [],
            "key_themes": [],
            "prioritized_keywords": []
        }

def annotate_resume_with_metadata(resume_text, line_metadata):
    """Annotate resume text with EXPLICIT character limits for each line.

    Instead of cryptic metadata, gives Gemini clear instructions:
    "Current: 45 chars, MAX ALLOWED: 55 chars, you can add UP TO 10 chars"
    """
    lines = resume_text.split('\n')
    annotated_lines = []

    for idx, line in enumerate(lines):
        line_stripped = line.strip()
        if not line_stripped:
            annotated_lines.append(line)
            continue

        # Find matching metadata (by text content)
        matching_meta = None
        for meta in line_metadata:
            if line_stripped in meta['text'] or meta['text'] in line_stripped:
                matching_meta = meta
                break

        if matching_meta:
            # Calculate explicit character limits
            current_length = matching_meta.get('current_length', len(line_stripped))
            char_buffer = matching_meta.get('char_buffer', 0)
            max_allowed = current_length + char_buffer
            visual_lines = matching_meta.get('visual_lines', 1)

            # Create CLEAR, EXPLICIT annotation
            if char_buffer == 0:
                limit_msg = f"‚ö†Ô∏è NO SPACE - Keep EXACTLY {current_length} chars or SHORTER"
            elif char_buffer <= 5:
                limit_msg = f"‚ö†Ô∏è TIGHT - Current: {current_length} chars, Max: {max_allowed} chars (only {char_buffer} chars available)"
            else:
                limit_msg = f"Current: {current_length} chars, Max: {max_allowed} chars (can add up to {char_buffer} chars)"

            annotation = f"\n[CHARACTER LIMIT: {limit_msg}]"
            annotated_lines.append(line + annotation)
        else:
            annotated_lines.append(line)

    return '\n'.join(annotated_lines)

def _normalize_for_match(text):
    """Normalize text to reduce unicode/whitespace variance for matching-only checks."""
    if text is None:
        return ''
    # Remove HTML-style tags first
    t = re.sub(r'<[bi]>', '', text)
    t = re.sub(r'</[bi]>', '', t)
    # Normalize unicode (NFKC), map NBSP to space, unify quotes/dashes, collapse whitespace
    t = unicodedata.normalize('NFKC', t)
    t = t.replace('\u00A0', ' ')
    t = t.replace('"', '"').replace('"', '"').replace(''', "'").replace(''', "'")
    t = t.replace('‚Äì', '-').replace('‚Äî', '-')
    t = re.sub(r"\s+", " ", t)
    return t.strip()

def _is_header_text(text):
    headers = {
        'PROFILE', 'SUMMARY', 'SKILLS', 'EDUCATION', 'WORK EXPERIENCE', 'EXPERIENCE',
        'PROJECTS', 'PUBLICATIONS', 'ACHIEVEMENTS', 'CERTIFICATIONS', 'CONTACT'
    }
    return text.strip().upper() in headers

def _is_low_content_change(original_text, updated_text):
    # Very short targets or nearly identical outputs are low-value
    if len(original_text.strip()) < 10:
        return True
    o = _normalize_for_match(original_text).lower()
    u = _normalize_for_match(updated_text).lower()
    if o == u:
        return True
    # If updated merely wraps original with <= 4 extra chars, consider low value
    if o in u and (len(u) - len(o)) <= 4:
        return True
    return False

def _contains_banned_phrases(text):
    banned_patterns = [
        r"\bshowcasing\b",
        r"\bdemonstrating\b",
        r"\bability to\b",
        r"\bproficient in modern development tools\b",
        r"\bresults[- ]driven\b",
        r"\bpassionate about\b"
    ]
    t = _normalize_for_match(text).lower()
    return any(re.search(p, t) for p in banned_patterns)

def _extracts_quantified_data(original_text):
    """Check if original text contains numbers, percentages, or metrics."""
    # Match numbers, percentages, dollar amounts, etc.
    quantified_patterns = [
        r'\d+%',  # Percentages: 40%, 99%
        r'\d+[kKmMbB]?',  # Numbers with optional K/M/B: 100, 5K, 2M
        r'\$\d+',  # Dollar amounts: $100K
        r'\d+x',  # Multipliers: 10x, 2x
        r'\d+\+',  # Plus notation: 50+
    ]
    return any(re.search(p, original_text) for p in quantified_patterns)

def _is_project_replacement(text):
    """Check if text is a complete project replacement (title + bullets)."""
    # Project replacement indicators:
    # 1. Has bold title: <b>...</b>
    # 2. Has bullet points: \n‚Ä¢ or \n‚Ä¢
    # 3. Multi-line content
    has_bold_title = bool(re.search(r'<b>.*?</b>', text))
    has_bullets = bool(re.search(r'\n[‚Ä¢‚óè]', text))
    is_multiline = '\n' in text
    
    return has_bold_title and has_bullets and is_multiline

def _removes_quantified_data(original_text, updated_text):
    """Check if updated text removes quantified data that was in original.
    
    EXCEPTION: If this is a complete PROJECT REPLACEMENT (entire project with title + bullets),
    allow metric removal since we're swapping entire projects, not just editing text.
    """
    # EXCEPTION: Complete project replacements are allowed to remove old metrics
    # because the new project will have its own metrics
    if _is_project_replacement(original_text) and _is_project_replacement(updated_text):
        # This is swapping one complete project for another - ALLOW IT
        # The old project's metrics don't apply to the new project
        return False
    
    # Extract numbers from both
    original_numbers = re.findall(r'\d+\.?\d*', original_text)
    updated_numbers = re.findall(r'\d+\.?\d*', updated_text)

    # If original had numbers but updated doesn't, that's bad
    if len(original_numbers) > 0 and len(updated_numbers) == 0:
        return True

    # If original had more numbers than updated, check if significant ones were removed
    if len(original_numbers) > len(updated_numbers):
        # Allow minor differences (like year changes), but flag major removals
        return True

    return False

def _validate_replacements(raw_replacements, line_metadata=None, safety_margin=1.0):
    """Validate replacements - trust Gemini but enforce hard constraints.

    Args:
        raw_replacements: List of replacement dictionaries
        line_metadata: Optional line metadata for character limit validation
        safety_margin: Use full buffer (1.0 = 100%) - trust Gemini's judgment
    """
    valid = []
    invalid = []

    # Create lookup for line metadata by text
    metadata_lookup = {}
    if line_metadata:
        for meta in line_metadata:
            # Store by normalized text for fuzzy matching
            normalized_text = _normalize_for_match(meta['text'])
            metadata_lookup[normalized_text] = meta

    for rep in raw_replacements:
        original_text = (rep.get('original_text') or '').strip()
        updated_text = (rep.get('updated_text') or '').strip()
        reason = None

        if not original_text or not updated_text:
            reason = 'missing_text'
        elif _is_header_text(original_text):
            reason = 'header_text'
        elif _contains_banned_phrases(updated_text):
            reason = 'banned_phrase'
        elif _is_low_content_change(original_text, updated_text):
            reason = 'low_content_change'
        elif _removes_quantified_data(original_text, updated_text):
            reason = 'removes_quantified_data'
        elif line_metadata:
            # Check character limit constraints - USE FULL BUFFER (trust Gemini)
            # Remove metadata annotations from original_text for comparison
            original_clean = re.sub(r'\[CHARACTER LIMIT:.*?\]', '', original_text, flags=re.DOTALL)
            original_clean = re.sub(r'\s*\[alignment=.*?\]', '', original_clean)
            normalized_original = _normalize_for_match(original_clean)

            # Find matching metadata
            matching_meta = None
            for norm_text, meta in metadata_lookup.items():
                if normalized_original in norm_text or norm_text in normalized_original:
                    matching_meta = meta
                    break

            if matching_meta:
                # Calculate character difference
                original_len = len(original_clean)
                updated_len = len(updated_text)
                char_diff = updated_len - original_len

                # Use FULL buffer - trust Gemini knows the limits
                full_buffer = matching_meta['char_buffer']

                # Only reject if it EXCEEDS the actual limit (not a safety margin)
                if char_diff > full_buffer:
                    reason = f'exceeds_char_limit (added {char_diff}, max allowed {full_buffer})'
            elif char_diff > 0:
                # No metadata found and it's an addition - be lenient, trust Gemini
                # Only reject if it's a huge addition (>50 chars)
                if char_diff > 50:
                    reason = 'no_metadata_found_large_addition'

        if reason:
            invalid.append({'original_text': original_text, 'reason': reason})
        else:
            valid.append({'original_text': original_text, 'updated_text': updated_text})

    return valid, invalid

def _parse_styled_text(text: str) -> Tuple[str, List[dict], List[dict]]:
    """Parse HTML-style tags and return plain text + style ranges + URL ranges.
    
    Returns:
        Tuple of (plain_text, style_ranges, url_ranges)
        - plain_text: Text without any tags
        - style_ranges: [{'start': int, 'end': int, 'bold': bool, 'italic': bool}]
        - url_ranges: [{'start': int, 'end': int, 'url': str}]
    """
    if not text:
        return '', [], []
    
    # Track style ranges and URL ranges
    styles = []
    urls = []
    plain_text = ''
    pos = 0
    text_pos = 0
    
    # Stack to track nested tags
    tag_stack = []
    
    while pos < len(text):
        # Look for URL tag: <url=https://example.com>
        if text[pos:pos+5] == '<url=':
            end_bracket = text.find('>', pos)
            if end_bracket != -1:
                url = text[pos+5:end_bracket]
                tag_stack.append({'type': 'url', 'start': text_pos, 'url': url})
                pos = end_bracket + 1
                continue
        # Look for closing URL tag: </url>
        elif text[pos:pos+6] == '</url>':
            # Close URL tag
            for i in range(len(tag_stack) - 1, -1, -1):
                if tag_stack[i]['type'] == 'url':
                    tag = tag_stack.pop(i)
                    urls.append({
                        'start': tag['start'],
                        'end': text_pos,
                        'url': tag['url']
                    })
                    break
            pos += 6
            continue
        # Look for opening bold tag
        elif text[pos:pos+3] == '<b>':
            tag_stack.append({'type': 'bold', 'start': text_pos})
            pos += 3
        # Look for opening italic tag
        elif text[pos:pos+3] == '<i>':
            tag_stack.append({'type': 'italic', 'start': text_pos})
            pos += 3
        # Look for closing bold tag
        elif text[pos:pos+4] == '</b>':
            # Close bold tag
            for i in range(len(tag_stack) - 1, -1, -1):
                if tag_stack[i]['type'] == 'bold':
                    tag = tag_stack.pop(i)
                    styles.append({
                        'start': tag['start'],
                        'end': text_pos,
                        'bold': True,
                        'italic': False
                    })
                    break
            pos += 4
        # Look for closing italic tag
        elif text[pos:pos+4] == '</i>':
            # Close italic tag
            for i in range(len(tag_stack) - 1, -1, -1):
                if tag_stack[i]['type'] == 'italic':
                    tag = tag_stack.pop(i)
                    styles.append({
                        'start': tag['start'],
                        'end': text_pos,
                        'bold': False,
                        'italic': True
                    })
                    break
            pos += 4
        else:
            # Regular character
            plain_text += text[pos]
            text_pos += 1
            pos += 1
    
    return plain_text, styles, urls

def _merge_style_ranges(style_ranges: List[dict]) -> List[dict]:
    """Merge duplicate ranges with the same start/end to combine bold/italic flags."""
    if not style_ranges:
        return []
    merged: dict = {}
    for r in style_ranges:
        key = (r.get('start'), r.get('end'))
        if key not in merged:
            merged[key] = {
                'start': r.get('start'),
                'end': r.get('end'),
                'bold': bool(r.get('bold')),
                'italic': bool(r.get('italic')),
            }
        else:
            merged[key]['bold'] = merged[key]['bold'] or bool(r.get('bold'))
            merged[key]['italic'] = merged[key]['italic'] or bool(r.get('italic'))
    # Return sorted by start then end for stable token insertion
    return sorted(merged.values(), key=lambda x: (x['start'], x['end']))



def _apply_styles_from_tags(docs_service, document_id, strip_tags: bool):
    """Apply styles using literal <b>/<i> tags present in the Doc.

    If strip_tags is True, remove the tags after styling; otherwise keep them.
    """
    # Read current plain text of document
    document = docs_service.documents().get(documentId=document_id).execute()
    content = document.get('body').get('content')
    doc_text = read_structural_elements_plain(content) or ''

    def find_tag_ranges(text: str, open_tag: str, close_tag: str) -> List[Tuple[int, int]]:
        ranges: List[Tuple[int, int]] = []
        stack: List[int] = []
        pos = 0
        open_len = len(open_tag)
        close_len = len(close_tag)
        while True:
            next_open = text.find(open_tag, pos)
            next_close = text.find(close_tag, pos)
            if next_open == -1 and next_close == -1:
                break
            if next_open != -1 and (next_close == -1 or next_open < next_close):
                # Push start of content after open tag
                stack.append(next_open + open_len)
                pos = next_open + open_len
            else:
                if stack:
                    start_content = stack.pop()
                    end_content = next_close
                    if end_content > start_content:
                        ranges.append((start_content, end_content))
                pos = next_close + close_len
        return ranges

    # Extract text snippets that should be bold/italic with their CLEAN versions
    # (without tags, as they will appear after tag stripping)
    bold_texts = []
    italic_texts = []

    for start, end in find_tag_ranges(doc_text, '<b>', '</b>'):
        if end > start:
            text_with_tags = doc_text[start:end]
            # Remove nested tags from the text we're searching for
            clean_text = text_with_tags.replace('<b>', '').replace('</b>', '').replace('<i>', '').replace('</i>', '')
            if clean_text.strip():
                bold_texts.append(clean_text)

    for start, end in find_tag_ranges(doc_text, '<i>', '</i>'):
        if end > start:
            text_with_tags = doc_text[start:end]
            clean_text = text_with_tags.replace('<b>', '').replace('</b>', '').replace('<i>', '').replace('</i>', '')
            if clean_text.strip():
                italic_texts.append(clean_text)

    print(f"Found {len(bold_texts)} text snippets to bold and {len(italic_texts)} to italicize")

    # Debug: print ALL bold texts we're looking for
    print("\nüìã All text to be bolded:")
    for idx, text in enumerate(bold_texts):
        print(f"  {idx+1}. '{text}'")

    # Save debug file showing what text has bold tags
    debug_bold_file = f"../Resumes/bold_debug_{uuid.uuid4().hex[:8]}.txt"
    with open(debug_bold_file, 'w', encoding='utf-8') as f:
        f.write("=== DOCUMENT WITH BOLD MARKERS ===\n\n")
        f.write("This shows what Gemini returned with <b> tags:\n\n")
        f.write(doc_text)
        f.write("\n\n=== LIST OF TEXT TO BE BOLDED ===\n\n")
        for idx, text in enumerate(bold_texts):
            f.write(f"{idx+1}. '{text}'\n")
    print(f"üìÑ Bold debug file saved to: {debug_bold_file}")

    # First, strip all tags from the document
    cleanup_requests = []
    for tok in ['<b>', '</b>', '<i>', '</i>']:
        cleanup_requests.append({
            'replaceAllText': {
                'containsText': {
                    'text': tok,
                    'matchCase': True
                },
                'replaceText': ''
            }
        })

    if cleanup_requests:
        docs_service.documents().batchUpdate(
            documentId=document_id,
            body={'requests': cleanup_requests}
        ).execute()
        print("Stripped tags from document")
        time.sleep(1)  # Wait for API to settle

    # Now apply formatting by finding the text snippets
    # Re-read the document after tag removal
    document = docs_service.documents().get(documentId=document_id).execute()
    content = document.get('body').get('content')
    clean_doc_text = read_structural_elements_plain(content) or ''

    format_requests = []

    # Apply bold formatting by finding each text snippet
    print("\nüé® Applying bold formatting:")
    for idx, text_to_bold in enumerate(bold_texts):
        text_position = clean_doc_text.find(text_to_bold)
        if text_position == -1:
            print(f"‚ùå {idx+1}. Could not find: '{text_to_bold}'")
            print(f"   Searching in: ...{clean_doc_text[max(0, text_position-50):text_position+100]}...")
            continue

        # Google Docs uses 1-based indexing
        # endIndex is EXCLUSIVE, so we need +2 (one for 1-based, one for exclusive end)
        start_idx = text_position + 1
        end_idx = text_position + len(text_to_bold) + 2

        # Show context
        context_start = max(0, text_position - 20)
        context_end = min(len(clean_doc_text), text_position + len(text_to_bold) + 20)
        context = clean_doc_text[context_start:context_end]

        print(f"‚úì {idx+1}. '{text_to_bold}' at position {text_position}")
        print(f"   Context: ...{context}...")
        print(f"   Range: [{start_idx}, {end_idx})")

        format_requests.append({
            'updateTextStyle': {
                'range': {
                    'startIndex': start_idx,
                    'endIndex': end_idx
                },
                'textStyle': {
                    'bold': True
                },
                'fields': 'bold'
            }
        })

    # Apply italic formatting
    for idx, text_to_italicize in enumerate(italic_texts):
        text_position = clean_doc_text.find(text_to_italicize)
        if text_position == -1:
            print(f"Warning: Could not find text to italicize: '{text_to_italicize[:30]}...'")
            continue

        # Same as bold - 1-based indexing with exclusive endIndex
        start_idx = text_position + 1
        end_idx = text_position + len(text_to_italicize) + 2

        print(f"Italicizing: '{text_to_italicize[:50]}...'")
        print(f"  Text position in doc: {text_position}, length: {len(text_to_italicize)}")
        print(f"  Applied range: [{start_idx}, {end_idx}) (endIndex is exclusive)")

        format_requests.append({
            'updateTextStyle': {
                'range': {
                    'startIndex': start_idx,
                    'endIndex': end_idx
                },
                'textStyle': {
                    'italic': True
                },
                'fields': 'italic'
            }
        })

    # Apply formatting in batches
    batch_size = 10
    for i in range(0, len(format_requests), batch_size):
        batch = format_requests[i:i + batch_size]
        if not batch:
            continue
        docs_service.documents().batchUpdate(
            documentId=document_id,
            body={'requests': batch}
        ).execute()
        print(f"Applied formatting batch {i//batch_size + 1}")

    print(f"‚úì Bold/italic formatting applied successfully!")

    # Create a visual debug file showing what SHOULD be bold
    debug_visual_file = f"../Resumes/bold_visual_{uuid.uuid4().hex[:8]}.txt"
    with open(debug_visual_file, 'w', encoding='utf-8') as f:
        f.write("=== VISUAL REPRESENTATION OF WHAT SHOULD BE BOLD ===\n\n")
        f.write("Legend: **text** = should be bold\n\n")

        # Create a marked-up version of the clean text
        visual_text = clean_doc_text

        # Sort bold_texts by position (longest first to avoid substring issues)
        bold_positions = []
        for text in bold_texts:
            pos = clean_doc_text.find(text)
            if pos != -1:
                bold_positions.append((pos, len(text), text))

        bold_positions.sort(reverse=True)  # Start from end to not mess up positions

        # Insert markers
        for pos, length, text in bold_positions:
            visual_text = visual_text[:pos] + '**' + text + '**' + visual_text[pos+length:]

        f.write(visual_text)

        f.write("\n\n=== SUMMARY ===\n")
        f.write(f"Total bold requests: {len(bold_texts)}\n")
        f.write(f"Successfully found: {len([t for t in bold_texts if clean_doc_text.find(t) != -1])}\n")
        f.write(f"Not found: {len([t for t in bold_texts if clean_doc_text.find(t) == -1])}\n")

        not_found = [t for t in bold_texts if clean_doc_text.find(t) == -1]
        if not_found:
            f.write("\nText that couldn't be found:\n")
            for text in not_found:
                f.write(f"  - '{text}'\n")

    print(f"üìÑ Visual bold debug saved to: {debug_visual_file}")

    # IMPORTANT: Read the actual document after formatting to see what's REALLY bold
    print("\nüîç Reading actual document to verify bold formatting...")
    time.sleep(2)  # Wait for formatting to settle

    final_document = docs_service.documents().get(documentId=document_id).execute()
    final_content = final_document.get('body', {}).get('content', [])

    # Create a debug file showing what's ACTUALLY bold in the document
    actual_bold_file = f"../Resumes/actual_bold_{uuid.uuid4().hex[:8]}.txt"
    with open(actual_bold_file, 'w', encoding='utf-8') as f:
        f.write("=== ACTUAL BOLD TEXT IN GOOGLE DOC ===\n\n")
        f.write("This shows what's actually bold in the final document:\n\n")

        for element in final_content:
            if 'paragraph' in element:
                para = element['paragraph']
                para_elements = para.get('elements', [])

                for elem in para_elements:
                    if 'textRun' in elem:
                        text_run = elem['textRun']
                        content = text_run.get('content', '')
                        text_style = text_run.get('textStyle', {})
                        is_bold = text_style.get('bold', False)

                        if is_bold:
                            f.write(f"**{content}**")
                        else:
                            f.write(content)

        f.write("\n\n=== SUMMARY ===\n")
        f.write("Text marked with **bold** is what's actually bold in the document.\n")
        f.write("Compare this with bold_visual_*.txt to see discrepancies.\n")

    print(f"üìÑ Actual bold formatting saved to: {actual_bold_file}")


def _regenerate_invalid_replacements(client, resume_text, job_description, invalid_items):
    if not invalid_items:
        return []
    # Ask model to only regenerate updated_text for the specified original_text entries
    originals_list = "\n".join([f"- {item['original_text']}" for item in invalid_items])
    prompt = f"""
You previously proposed replacements for tailoring a resume, but some items violated constraints
(generic language, headers, newlines, removed quantified data, or low-value changes). 

For the list below, regenerate ONLY the updated_text values. Keep original_text exactly the same.

‚ö†Ô∏è  CRITICAL - PRESERVE ALL QUANTIFIED DATA:
- If original has "95%", your updated_text MUST include "95%"
- If original has "team of 3", your updated_text MUST include "3" or "team of 3"
- If original has "1,500+", your updated_text MUST include "1,500+"
- If original has "70% and 30%", your updated_text MUST include BOTH "70%" AND "30%"
- ONLY change words AROUND the numbers, NEVER remove the numbers!

EXAMPLES:
‚úÖ GOOD: "Led team of 3 to design RAG engine" ‚Üí "Led team of 3 building RAG engine for document analysis"
‚úÖ GOOD: "70% expanded query + 30% RAG" ‚Üí "hybrid approach (70% query expansion + 30% RAG retrieval)"
‚ùå BAD: "Led team of 3" ‚Üí "Led cross-functional team" (REMOVED the number 3!)
‚ùå BAD: "70% expanded + 30% RAG" ‚Üí "hybrid retrieval pipeline" (REMOVED both percentages!)

Other Constraints:
- No generic phrases (e.g., demonstrating, showcasing, ability to)
- No newlines, bullets, or headers
- Make concrete, job-relevant improvements using the job description
- Preserve professional tone and be specific
- 1 replacement per original_text
- You can use <b>text</b> for bold and <i>text</i> for italic styling if needed

Return strict JSON with this shape:
{{"replacements": [{{"original_text": "...", "updated_text": "..."}}]}}

Items to fix:
{originals_list}

RESUME:
{resume_text}

JOB DESCRIPTION:
{job_description}
"""
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=prompt
    )
    text = response.candidates[0].content.parts[0].text.strip()
    if text.startswith('```'):
        text = text.replace('```json', '').replace('```', '').strip()
    try:
        data = json.loads(text)
        return data.get('replacements', [])
    except Exception:
        return []

def apply_json_replacements_to_doc(docs_service, document_id, replacements_json, resume_text=None, job_description_text=None, keep_tags_literal=None, line_metadata=None):
    """Applies text replacements from JSON to Google Doc while preserving all formatting.

    Args:
        docs_service: Google Docs API service
        document_id: Document ID to apply replacements to
        replacements_json: JSON string with replacements
        resume_text: Optional original resume text for regeneration
        job_description_text: Optional job description for regeneration
        keep_tags_literal: Optional flag to keep HTML tags visible
        line_metadata: Optional line metadata for character limit validation
    """
    try:
        # Parse the JSON response from Gemini
        try:
            # Extract JSON from markdown code blocks if present
            json_text = replacements_json.strip()
            if json_text.startswith('```json'):
                # Remove markdown code block formatting
                json_text = json_text.replace('```json', '').replace('```', '').strip()
            elif json_text.startswith('```'):
                # Handle generic code blocks
                json_text = json_text.replace('```', '').strip()

            # Fix common JSON issues from Gemini
            # 1. Replace literal newlines in strings with escaped newlines
            # This is tricky - we need to escape newlines that are INSIDE string values
            # but not structural newlines in the JSON itself

            try:
                # First attempt: try normal JSON parsing
                replacements_data = json.loads(json_text)
            except json.JSONDecodeError as e:
                # If it fails, likely due to unescaped newlines in strings
                # Try to fix it by escaping control characters
                print(f"‚ö†Ô∏è  JSON parse error, attempting to fix control characters...")

                # Replace common control characters that break JSON
                json_text_fixed = json_text.replace('\n"', '\\n"')  # Newline before closing quote
                json_text_fixed = json_text_fixed.replace('"\n', '"\\n')  # Newline after opening quote

                # Try parsing again
                try:
                    replacements_data = json.loads(json_text_fixed)
                    print("‚úÖ Fixed JSON by escaping control characters")
                except:
                    # Still failing - use more aggressive fix
                    print("‚ö†Ô∏è  Attempting aggressive JSON repair...")
                    import re

                    # Find all string values and escape newlines within them
                    def escape_newlines_in_strings(match):
                        string_content = match.group(1)
                        # Escape newlines and other control chars
                        string_content = string_content.replace('\n', '\\n')
                        string_content = string_content.replace('\r', '\\r')
                        string_content = string_content.replace('\t', '\\t')
                        return f'"{string_content}"'

                    # Pattern to match string values in JSON
                    json_text_fixed = re.sub(r'"([^"]*)"', escape_newlines_in_strings, json_text)

                    try:
                        replacements_data = json.loads(json_text_fixed)
                        print("‚úÖ Fixed JSON with aggressive repair")
                    except Exception as final_error:
                        print(f"‚ùå Could not repair JSON: {final_error}")
                        print(f"Raw JSON (first 500 chars): {json_text[:500]}")
                        raise
            replacements = replacements_data.get('replacements', [])

            # Extract line tracking info if present
            lines_added = replacements_data.get('lines_added', 0)
            lines_freed = replacements_data.get('lines_freed', 0)
            net_lines = replacements_data.get('net_lines', 0)

            if net_lines > 0:
                print(f"üìä Line budget tracking: Added {lines_added}, Freed {lines_freed}, Net {net_lines}")

        except json.JSONDecodeError as e:
            print(f"Error parsing JSON response: {e}")
            print(f"Raw response: {replacements_json}")
            return

        if not replacements:
            print("No replacements needed - resume is already well-tailored!")
            return

        print(f"Found {len(replacements)} text replacements from model")

        # Validate - trust Gemini but enforce hard constraints only
        client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
        valid, invalid = _validate_replacements(replacements, line_metadata=line_metadata, safety_margin=1.0)

        print(f"üîç Validation Results: {len(valid)} valid, {len(invalid)} invalid replacements (trusting Gemini, only hard limits enforced)")
        
        if invalid:
            print(f"‚ö†Ô∏è  Invalid items detected:")
            for item in invalid:
                print(f"   - '{item['original_text'][:50]}...' (Reason: {item['reason']})")
            
            if resume_text is not None and job_description_text is not None:
                print(f"üîÑ Attempting to regenerate {len(invalid)} invalid items...")
                regenerated = _regenerate_invalid_replacements(client, resume_text, job_description_text, invalid)
                regenerated_valid, regenerated_invalid = _validate_replacements(regenerated)
                valid.extend(regenerated_valid)
                print(f"‚úÖ Regeneration complete: {len(regenerated_valid)} successful, {len(regenerated_invalid)} still invalid")
                
                if regenerated_invalid:
                    print(f"‚ùå Items that couldn't be regenerated:")
                    for item in regenerated_invalid:
                        print(f"   - '{item['original_text'][:50]}...' (Reason: {item['reason']})")
            else:
                print("‚ö†Ô∏è  Cannot regenerate: missing resume_text or job_description_text")
        else:
            print("‚úÖ All replacements passed validation - no regeneration needed")

        if not valid:
            print("No valid replacements after validation. Aborting.")
            return

        # Score and prioritize replacements (best first)
        print(f"\nüìä Scoring and prioritizing {len(valid)} valid replacements...")
        scored_replacements = []
        for rep in valid:
            score = score_replacement(rep, line_metadata)
            if score >= 0:  # Only include non-negative scores
                scored_replacements.append({
                    'replacement': rep,
                    'score': score
                })
        
        # Sort by score (highest first)
        scored_replacements.sort(key=lambda x: x['score'], reverse=True)
        
        # Trust Gemini completely - apply ALL valid replacements!
        # Gemini has the character limits and context, so its suggestions are optimal
        print(f"‚úÖ Applying ALL {len(scored_replacements)} replacements from Gemini (trusting AI judgment)")
        
        # Show prioritized list
        print(f"üìã Prioritized replacements (top {len(scored_replacements)}):")
        for i, item in enumerate(scored_replacements, 1):
            rep = item['replacement']
            score = item['score']
            orig_preview = rep['original_text'][:40]
            print(f"  {i}. Score {score:.1f}: '{orig_preview}...'")
        
        # Extract just the replacements
        valid = [item['replacement'] for item in scored_replacements]

        # Read doc plain text to count occurrences (use plain version for matching)
        document = docs_service.documents().get(documentId=document_id).execute()
        doc_text = read_structural_elements_plain(document.get('body').get('content')) or ''
        doc_text_norm = _normalize_for_match(doc_text)

        api_requests = []
        style_requests = []
        marker_specs_all = []

        # Determine whether to keep tags literally in the doc (debug inspection mode)
        if keep_tags_literal is None:
            keep_tags_literal = os.getenv('KEEP_TAGS_LITERAL', 'false').lower() in ('1', 'true', 'yes')
        
        for item in valid:
            original_text = item['original_text']
            updated_text = item['updated_text']

            # Check if this is a multi-line replacement (project replacement)
            is_multiline = '\n' in original_text or '\n' in updated_text

            if is_multiline:
                print(f"üì¶ Multi-line replacement detected (project/section replacement)")
                print(f"   Original lines: {original_text.count(chr(10)) + 1}")
                print(f"   Updated lines: {updated_text.count(chr(10)) + 1}")

            # Parse styling from updated_text
            plain_updated_text, style_ranges, url_ranges = _parse_styled_text(updated_text)

            # Remove styling tags from original_text for matching against plain doc_text
            plain_original_text, _, _ = _parse_styled_text(original_text)

            # Ensure exact, unique occurrence; eliminate fuzzy matching
            count = doc_text.count(plain_original_text)
            if count == 1:
                if keep_tags_literal:
                    # Insert updated_text verbatim (with <b>/<i> visible) for inspection
                    api_requests.append({
                        'replaceAllText': {
                            'containsText': {
                                'text': plain_original_text,
                                'matchCase': True
                            },
                            'replaceText': updated_text
                        }
                    })
                    print(f"Queued literal-tag replace for: '{plain_original_text[:60]}...'")
                else:
                    # ALWAYS use plain text (no tags) in replaceAllText
                    api_requests.append({
                        'replaceAllText': {
                            'containsText': {
                                'text': plain_original_text,
                                'matchCase': True
                            },
                            'replaceText': plain_updated_text
                        }
                    })

                    # Store formatting info if there are style ranges or URL ranges
                    if style_ranges or url_ranges:
                        marker_specs_all.append({
                            'text': plain_updated_text,
                            'style_ranges': style_ranges,
                            'url_ranges': url_ranges
                        })
                        print(f"Queued plain replace for: '{plain_original_text[:60]}...' (with {len(style_ranges)} style ranges, {len(url_ranges)} URL ranges)")
                        print(f"  ‚Üí Will format: '{plain_updated_text[:60]}...'")
                        print(f"  ‚Üí Updated text (with tags): '{updated_text[:80]}...'")
                        print(f"  ‚Üí Style ranges: {style_ranges}")
                        if url_ranges:
                            print(f"  ‚Üí URL ranges: {url_ranges}")
                    else:
                        print(f"Queued plain replace for: '{plain_original_text[:60]}...'")
            elif count == 0:
                # Attempt a normalized check to warn about unicode issues (no replacement applied)
                if _normalize_for_match(plain_original_text) in doc_text_norm:
                    print(f"Warning: normalized match found but exact text not present; skipping: '{plain_original_text[:60]}...'")
                else:
                    print(f"! Could not find match for: '{plain_original_text[:60]}...' ‚Äî skipping")
            else:
                print(f"Warning: multiple ({count}) occurrences of target text; skipping to avoid over-replacement")

        print(f"Applying {len(api_requests)} replaceAllText operations")

        # Apply text replacements in batches (preserving existing formatting)
        batch_size = 10
        for i in range(0, len(api_requests), batch_size):
            batch = api_requests[i:i + batch_size]
            if batch:
                docs_service.documents().batchUpdate(
                    documentId=document_id,
                    body={'requests': batch}
                ).execute()
                print(f"Applied batch {i//batch_size + 1}")

        print("‚úì Text replacements applied successfully")

        # Now apply formatting if we have marker specs
        if marker_specs_all:
            print(f"\nüé® Applying bold/italic/URL formatting using Google Docs API structure...")
            print(f"üìã Debug: marker_specs_all contains {len(marker_specs_all)} items:")
            for i, spec in enumerate(marker_specs_all):
                print(f"  {i+1}. Text: '{spec['text'][:60]}...'")
                print(f"      Style ranges: {spec.get('style_ranges', [])}")
                print(f"      URL ranges: {spec.get('url_ranges', [])}")
            time.sleep(1)  # Wait for replacements to settle

            # Re-read the document to get the updated structure
            doc_after_replace = docs_service.documents().get(documentId=document_id).execute()
            print(f"üîç Debug: doc_after_replace keys: {doc_after_replace.keys()}")
            body = doc_after_replace.get('body', {})
            print(f"üîç Debug: body keys: {body.keys() if body else 'None'}")
            content_after = body.get('content', [])
            print(f"üîç Debug: content_after type: {type(content_after)}, length: {len(content_after)}")

            format_requests = []

            # Build complete document text with index mapping
            # Text can span multiple textRuns, so we need to reconstruct the full text
            # We manually track the cumulative index since startIndex may not be present after replacements
            doc_text_parts = []
            cumulative_index = 1  # Google Docs uses 1-based indexing
            
            print(f"üîç Debug: content_after has {len(content_after)} elements")
            for idx, element in enumerate(content_after):
                if 'paragraph' in element:
                    para = element['paragraph']
                    para_elements = para.get('elements', [])
                    if idx < 5:  # Only show first 5 for brevity
                        print(f"üîç Debug: Element {idx} is paragraph with {len(para_elements)} elements")
                    for elem in para_elements:
                        if 'textRun' in elem:
                            text_run = elem['textRun']
                            content = text_run.get('content', '')
                            # Use provided startIndex if available, otherwise use our cumulative tracker
                            start_index = text_run.get('startIndex')
                            if start_index is None:
                                start_index = cumulative_index
                            
                            if idx < 5 and len(doc_text_parts) < 10:  # Only show first few
                                print(f"üîç Debug: Found textRun with startIndex={start_index}, content[:30]='{content[:30]}'")
                            
                            doc_text_parts.append({
                                'content': content,
                                'start_index': start_index
                            })
                            
                            # Update cumulative index for next textRun
                            cumulative_index = start_index + len(content)
            
            print(f"üîç Debug: Collected {len(doc_text_parts)} text parts")

            # Reconstruct full document text
            full_doc_text = ''.join([part['content'] for part in doc_text_parts])

            print(f"\nüîç Debug: Full document text length: {len(full_doc_text)}")
            print(f"üîç Debug: First 200 chars: '{full_doc_text[:200]}'")

            # Track text ranges that need formatting cleared first
            clear_formatting_requests = []

            for spec in marker_specs_all:
                target_text = spec['text']
                style_ranges = spec.get('style_ranges', [])
                url_ranges = spec.get('url_ranges', [])

                # Search for target text in full document
                text_position = full_doc_text.find(target_text)
                if text_position == -1:
                    print(f"‚ö†Ô∏è  Could not find text to format: '{target_text[:50]}...'")
                    # Debug: show where we're searching
                    print(f"   üîç Debug: Searching for text with length {len(target_text)}")
                    print(f"   üîç Debug: First 100 chars of target: '{target_text[:100]}'")
                    # Try partial match
                    if target_text[:30] in full_doc_text:
                        print(f"   ‚úì Found first 30 chars in document!")
                        pos = full_doc_text.find(target_text[:30])
                        print(f"   Context: ...{full_doc_text[max(0,pos-20):pos+80]}...")
                    else:
                        print(f"   ‚úó Not even first 30 chars found in document")
                    continue

                # Find the Google Docs startIndex for this position
                # Walk through doc_text_parts to find which textRun contains this position
                cumulative_length = 0
                found_start_index = None

                for part in doc_text_parts:
                    part_length = len(part['content'])
                    if cumulative_length <= text_position < cumulative_length + part_length:
                        # Found the textRun containing the start of our text
                        offset_in_run = text_position - cumulative_length
                        found_start_index = part['start_index'] + offset_in_run
                        break
                    cumulative_length += part_length

                if found_start_index is None:
                    print(f"‚ö†Ô∏è  Could not determine document index for: '{target_text[:50]}...'")
                    continue

                # STEP 1: Clear all existing formatting from this text range
                # This ensures old bold/italic formatting doesn't persist
                # Note: Links are automatically cleared by text replacement, so we only clear bold/italic
                text_range_start = found_start_index
                text_range_end = found_start_index + len(target_text)
                
                # Clear bold and italic only (links are handled separately)
                clear_formatting_requests.append({
                    'updateTextStyle': {
                        'range': {
                            'startIndex': text_range_start,
                            'endIndex': text_range_end
                        },
                        'textStyle': {
                            'bold': False,
                            'italic': False
                        },
                        'fields': 'bold,italic'
                    }
                })
                print(f"  üßπ Clearing old bold/italic from: '{target_text[:50]}...' at [{text_range_start}, {text_range_end})")

                # STEP 2: Apply URL links (must be done before bold/italic for proper formatting)
                for url_range in url_ranges:
                    range_start = found_start_index + url_range['start']
                    range_end = found_start_index + url_range['end']
                    url = url_range['url']
                    
                    format_requests.append({
                        'updateTextStyle': {
                            'range': {
                                'startIndex': range_start,
                                'endIndex': range_end
                            },
                            'textStyle': {
                                'link': {
                                    'url': url
                                }
                            },
                            'fields': 'link'
                        }
                    })
                    print(f"  URL: '{target_text[url_range['start']:url_range['end']]}' ‚Üí {url} at [{range_start}, {range_end})")

                # Apply each style range within this text
                for style_range in style_ranges:
                    range_start = found_start_index + style_range['start']
                    range_end = found_start_index + style_range['end']
                    
                    # Extract the text to be formatted
                    text_to_format = target_text[style_range['start']:style_range['end']]

                    if style_range['bold']:
                        # Apply bold formatting to the entire range (including numbers)
                        format_requests.append({
                            'updateTextStyle': {
                                'range': {
                                    'startIndex': range_start,
                                    'endIndex': range_end
                                },
                                'textStyle': {'bold': True},
                                'fields': 'bold'
                            }
                        })
                        print(f"  Bold: '{text_to_format}' at [{range_start}, {range_end})")

                    if style_range['italic']:
                        # Apply italic formatting to the entire range (including numbers)
                        format_requests.append({
                            'updateTextStyle': {
                                'range': {
                                    'startIndex': range_start,
                                    'endIndex': range_end
                                },
                                'textStyle': {'italic': True},
                                'fields': 'italic'
                            }
                        })
                        print(f"  Italic: '{text_to_format}' at [{range_start}, {range_end})")

            # STEP 1: Clear old formatting first
            if clear_formatting_requests:
                print(f"\nüßπ Clearing old formatting from {len(clear_formatting_requests)} text ranges...")
                batch_size = 10
                for i in range(0, len(clear_formatting_requests), batch_size):
                    batch = clear_formatting_requests[i:i + batch_size]
                    docs_service.documents().batchUpdate(
                        documentId=document_id,
                        body={'requests': batch}
                    ).execute()
                print(f"‚úÖ Cleared formatting from {len(clear_formatting_requests)} ranges")
                time.sleep(0.5)  # Brief pause to let clearing settle

            # STEP 2: Apply new formatting
            if format_requests:
                print(f"\n‚ú® Applying new formatting to {len(format_requests)} ranges...")
                batch_size = 10
                for i in range(0, len(format_requests), batch_size):
                    batch = format_requests[i:i + batch_size]
                    docs_service.documents().batchUpdate(
                        documentId=document_id,
                        body={'requests': batch}
                    ).execute()
                print(f"‚úÖ Applied {len(format_requests)} formatting operations")
            else:
                print("No formatting to apply")
        else:
            print("No formatting specified")
        
    except HttpError as error:
        print(f"An error occurred while applying changes: {error}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()

def download_doc_as_pdf(drive_service, doc_id, pdf_path):
    """Downloads a Google Doc as a PDF file."""
    try:
        request = drive_service.files().export_media(fileId=doc_id, mimeType='application/pdf')
        with open(pdf_path, 'wb') as f:
            f.write(request.execute())
        print(f"Successfully downloaded tailored resume to {pdf_path}")
    except HttpError as error:
        print(f"An error occurred during PDF download: {error}")

def tailor_resume(resume_text, job_description, line_metadata=None, line_budget=None, keywords=None, mimikree_data=None, previous_validation=None, iteration=1):
    """Enhanced resume tailoring with format preservation and keyword optimization.

    Args:
        resume_text: Original resume text (possibly annotated with metadata)
        job_description: Job description text
        previous_validation: Previous iteration's validation report (if any)
        iteration: Current iteration number
        line_metadata: Optional line metadata from extract_document_structure
        line_budget: Optional line budget from calculate_line_budget
        keywords: Optional keywords from extract_job_keywords
        mimikree_data: Optional formatted Mimikree profile information
    """
    client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

    # Build format constraints section
    format_constraints = ""
    if line_budget:
        format_constraints = f"""
FORMAT PRESERVATION CONSTRAINTS (CRITICAL - MUST FOLLOW):
- Current resume: {line_budget['current_visual_lines']} visual lines (some paragraphs wrap to multiple lines)
- Available lines to add: {line_budget['available_lines']} (STRICT: DO NOT EXCEED)
- Each paragraph shows [char_buffer=X, visual_lines=Y]
  * char_buffer: MAX characters you can ADD before causing text to wrap to next line
  * visual_lines: How many lines this paragraph currently spans
- Example: "Led team [char_buffer=10, visual_lines=1]" ‚Üí Can add up to 10 chars
- DO NOT remove or reduce quantified data (numbers, percentages, metrics)
  * WRONG: "Improved performance by 40%" ‚Üí "Improved performance significantly"
  * RIGHT: "Improved performance by 40%" ‚Üí "Optimized performance by 40%"
- PRESERVE original resume length - if it was 1 page, keep it 1 page
- Text that naturally wraps is ONE paragraph - don't treat wrapped lines as separate items
"""

    # Build keyword prioritization section
    keyword_section = ""
    if keywords:
        required_skills_str = ", ".join(keywords.get('required_skills', []))
        key_themes_str = ", ".join(keywords.get('key_themes', []))
        prioritized_str = ", ".join(keywords.get('prioritized_keywords', [])[:10])  # Top 10

        keyword_section = f"""
KEYWORD OPTIMIZATION STRATEGY:
- REQUIRED SKILLS TO EMPHASIZE: {required_skills_str}
- KEY THEMES TO HIGHLIGHT: {key_themes_str}
- TOP PRIORITY KEYWORDS: {prioritized_str}

STRATEGIC PLACEMENT:
- Place content matching required skills and key themes EARLY in relevant sections
- If job emphasizes "leadership", move leadership accomplishments to TOP of experience bullets
- If job needs "Python", ensure Python experience appears prominently
- Reorder bullets within sections to showcase most relevant achievements first
- Use exact keywords from the job description when replacing text
"""

    # Build Mimikree profile section
    mimikree_section = ""
    if mimikree_data:
        mimikree_section = f"""
ADDITIONAL USER PROFILE INFORMATION (From Mimikree):
The following information was gathered from the user's personal AI assistant (Mimikree) which has comprehensive knowledge of their background, skills, projects, and achievements. Use this information to enhance the resume with specific, relevant details.

{mimikree_data}

IMPORTANT:
- Use this information to add concrete details and examples to the resume
- Reference specific projects, skills, and achievements mentioned in the Mimikree responses
- Ensure all additions align with the job requirements
- Maintain consistency with the existing resume content
"""

    # Build validation feedback section (if this is a retry iteration)
    validation_feedback = ""
    if previous_validation and iteration > 1:
        validation_feedback = f"""
‚ö†Ô∏è QUALITY ISSUES FROM PREVIOUS ITERATION (ITERATION {iteration} - FIX THESE):

The previous tailoring attempt (iteration {iteration-1}) had quality issues. You MUST address these in this iteration:

üìä PREVIOUS SCORES:
- Overall Score: {previous_validation.get('overall_score', 0):.1f}/100 (Target: 80+)
- Line Quality: {previous_validation.get('line_quality_score', 0):.1f}/100
- Content Quality: {previous_validation.get('content_quality_score', 0):.1f}/100
- ATS Score: {previous_validation.get('ats_score', 0):.1f}/100

"""
        
        # Add line quality issues
        line_issues = previous_validation.get('line_quality_issues', [])
        if line_issues:
            validation_feedback += "üî¥ LINE QUALITY ISSUES (FIX THESE):\n"
            for issue in line_issues[:10]:  # Show top 10
                if issue['type'] == 'orphaned_word':
                    validation_feedback += f"  ‚Ä¢ Line {issue['line_num']}: ORPHANED WORD '{issue['word']}' - Wrap it with previous line!\n"
                elif issue['type'] == 'short_line':
                    validation_feedback += f"  ‚Ä¢ Line {issue['line_num']}: TOO SHORT ({issue['words']} words) - Combine with previous line or add detail\n"
                elif issue['type'] == 'low_utilization':
                    validation_feedback += f"  ‚Ä¢ Line {issue['line_num']}: Only {issue['utilization']:.0f}% filled - Add more detail or combine lines\n"
                elif issue['type'] == 'awkward_ending':
                    validation_feedback += f"  ‚Ä¢ Line {issue['line_num']}: AWKWARD ending with '{issue['word']}' - Rewrite to end stronger\n"
            validation_feedback += "\n"
        
        # Add content quality issues
        content_issues = previous_validation.get('content_quality_issues', [])
        if content_issues:
            validation_feedback += "üî¥ CONTENT QUALITY ISSUES (IMPROVE THESE BULLETS):\n"
            for issue in content_issues[:8]:  # Show top 8
                validation_feedback += f"  ‚Ä¢ Line {issue['line_num']}: Score {issue['score']:.0f}/100\n"
                validation_feedback += f"    Text: \"{issue['text']}\"\n"
                if issue.get('suggestions'):
                    validation_feedback += f"    Fix: {', '.join(issue['suggestions'][:2])}\n"
            validation_feedback += "\n"
        
        # Add ATS issues
        ats_issues = previous_validation.get('ats_issues', {})
        missing_keywords = ats_issues.get('missing_critical_keywords', [])
        if missing_keywords:
            validation_feedback += f"üî¥ MISSING CRITICAL KEYWORDS (ADD THESE):\n"
            for kw in missing_keywords[:15]:  # Top 15
                validation_feedback += f"  ‚Ä¢ '{kw}' - MUST appear in resume!\n"
            validation_feedback += "\n"
        
        weak_bullets = ats_issues.get('weak_keyword_bullets', [])
        if weak_bullets:
            validation_feedback += f"üî¥ BULLETS WITH POOR KEYWORD COVERAGE (ENHANCE THESE):\n"
            for bullet in weak_bullets[:5]:  # Top 5
                validation_feedback += f"  ‚Ä¢ Line {bullet['line_num']}: \"{bullet['text'][:60]}...\" (only {bullet['keyword_count']} keywords)\n"
            validation_feedback += "\n"
        
        validation_feedback += f"""
üéØ YOUR MISSION FOR THIS ITERATION:
1. FIX all orphaned words by wrapping with previous lines
2. IMPROVE low-scoring bullets (use strong action verbs, add impact, quantify results)
3. ADD missing critical keywords naturally into relevant bullet points
4. ENHANCE keyword density in weak bullets
5. ELIMINATE short/underutilized lines by combining or expanding them
6. MAKE SURE all bullets end with strong nouns/verbs, not weak words like "and", "to", "of"

TARGET: Achieve 80+ overall score to pass quality gates.
"""

    prompt = f"""Tailor this resume for the job while preserving formatting. Make strategic replacements using job keywords.

‚ö†Ô∏è CRITICAL - CHARACTER LIMITS ARE ABSOLUTE:
Each line in the resume has a CHARACTER LIMIT annotation showing:
- Current character count
- Maximum allowed characters
- Available buffer

YOU MUST respect these limits. If a line says "Max: 55 chars", your replacement MUST be ‚â§ 55 characters.
If it says "NO SPACE", make it SHORTER or keep same length.

EXAMPLES:
‚úì Original: "Built AI system" (Current: 15 chars, Max: 25 chars)
  Valid: "Developed ML pipeline" (21 chars) ‚úì
  Valid: "Created AI model" (16 chars) ‚úì
  INVALID: "Built comprehensive AI system" (29 chars) ‚úó EXCEEDS LIMIT!

‚úì Original: "Improved accuracy by 95%" (Current: 60 chars, Max: 60 chars - NO SPACE)
  Valid: "Enhanced accuracy by 95%" (24 chars - SHORTER) ‚úì
  Valid: "Boosted model accuracy by 95%" (29 chars - SHORTER) ‚úì
  INVALID: "Significantly improved accuracy by 95%" (38 chars) - Still fine if < 60!

CRITICAL RULES - QUANTIFIED DATA PRESERVATION:
‚ö†Ô∏è  NEVER REMOVE OR MODIFY ANY NUMBERS, PERCENTAGES, OR METRICS!
- If original says "95%", your replacement MUST contain "95%"
- If original says "team of 3", your replacement MUST contain "3" or "team of 3"
- If original says "1,500+ documents", your replacement MUST contain "1,500+"
- If original says "70% expanded query + 30% RAG", KEEP ALL NUMBERS: "70%", "30%"
- Only change the WORDS AROUND the numbers, NEVER the numbers themselves

EXAMPLES OF CORRECT QUANTIFIED DATA PRESERVATION:
‚úÖ GOOD: "Improved accuracy by 95% through automation" ‚Üí "Enhanced accuracy by 95% via data analysis"
‚úÖ GOOD: "Led team of 3 engineers" ‚Üí "Managed team of 3 developing solutions"
‚úÖ GOOD: "Processed 1,500+ documents" ‚Üí "Analyzed 1,500+ documents for insights"
‚úÖ GOOD: "70% expanded + 30% RAG" ‚Üí "hybrid pipeline (70% query expansion + 30% RAG)"

‚ùå BAD: "Improved accuracy by 95%" ‚Üí "Significantly improved accuracy" (REMOVED 95%)
‚ùå BAD: "Led team of 3" ‚Üí "Led cross-functional team" (REMOVED 3)
‚ùå BAD: "70% expanded + 30% RAG" ‚Üí "hybrid retrieval approach" (REMOVED percentages)

OTHER CRITICAL RULES:
- Use EXACT text matches from resume (ignore [metadata] annotations)
- Respect char_buffer limits (max chars you can add per line)
- No generic phrases, headers, or contact info changes
- Use <b> and <i> tags for bold/italic styling if needed
- Use <url=https://example.com>text</url> tags to preserve URLs/links
  * Example: <url=https://github.com/user>github/user</url>
  * ALWAYS preserve URLs from the original text - copy the URL exactly!
  * If original has "<url=https://mimikree.com>mimikree.com</url>", keep the URL in your replacement
- UNLIMITED replacements - make ALL high-impact changes needed
- You CAN replace entire project descriptions if a better project is available
- You CAN reorder/replace skills in Skills section to match job requirements
- Focus on RELEVANCE to this specific job over everything else

    {format_constraints}

{keyword_section}

{mimikree_section}

{validation_feedback}

WHAT YOU CAN CHANGE:

1. BULLET POINTS & DESCRIPTIONS (always preserve numbers):
   ‚úÖ "Developed web applications" ‚Üí "Built data analysis dashboards"
   ‚úÖ "Created automation tools" ‚Üí "Designed experimentation frameworks"
   ‚úÖ "Improved system performance by 40%" ‚Üí "Enhanced ML model performance by 40%"

2. ENTIRE PROJECTS (if Mimikree data shows better alternatives):
   ‚ö†Ô∏è  CRITICAL: When replacing a project, you MUST replace the project title AND all its bullets together!

   FORMAT FOR PROJECT REPLACEMENT (CRITICAL - USE ESCAPED NEWLINES):
   {{
       "original_text": "<b>Project Title</b>\\n‚Ä¢ Bullet 1\\n‚Ä¢ Bullet 2\\n‚Ä¢ Bullet 3",
       "updated_text": "<b>New Project Title</b>\\n‚Ä¢ New bullet 1\\n‚Ä¢ New bullet 2\\n‚Ä¢ New bullet 3"
   }}

   ‚ö†Ô∏è IMPORTANT: In JSON, newlines MUST be escaped as \\n (double backslash + n)
   NOT as literal line breaks!

   EXAMPLE:
   ‚úÖ CORRECT - Replace entire project atomically:
   Original: "<b>E-commerce Website</b>\\n‚Ä¢ Built shopping cart\\n‚Ä¢ Added user auth"
   Updated:  "<b>ML Recommendation Engine</b>\\n‚Ä¢ Built recommendation model with 95% accuracy\\n‚Ä¢ Increased sales by 40%"

   ‚ùå WRONG - Don't replace title and bullets separately:
   Don't do: Replace title only, leaving old bullets
   Don't do: Replace one bullet but not the others

   ‚úÖ You CAN also just modify individual bullets if the project is relevant:
   Original: "‚Ä¢ Built shopping cart with React"
   Updated:  "‚Ä¢ Built data pipeline with Python"

3. SKILLS SECTION:
   ‚úÖ Reorder skills to put job-required skills first
   ‚úÖ Replace less relevant skills with more relevant ones from Mimikree data
   ‚úÖ Example: Move "Python, SQL, Machine Learning" to top if job emphasizes data science

4. PROFILE/SUMMARY:
   ‚úÖ Adjust focus and keywords to match job requirements
   ‚úÖ Example: "Full-stack developer" ‚Üí "Data Scientist specializing in ML"

WHAT YOU CANNOT CHANGE:
‚ùå Any numbers, percentages, metrics (95%, 3, 1500+, 70%, $100K, 5 years, etc.)
‚ùå Headers (PROFILE, EDUCATION, EXPERIENCE, PROJECTS, SKILLS)
‚ùå Contact information
‚ùå Company names, job titles, dates (in work experience)

CHARACTER LIMIT EXAMPLES:
Good: "Developed web apps [char_buffer=12]" ‚Üí "Developed React apps" (+6 chars, under limit)
Bad: "Managed team [char_buffer=5]" ‚Üí "Managed cross-functional team" (+20 chars - EXCEEDS limit!)

JSON FORMAT:
{{
    "replacements": [
        {{
            "original_text": "exact text from resume",
            "updated_text": "improved version with keywords",
            "reason": "why this improves fit"
        }}
    ],
    "lines_added": 0,
    "lines_freed": 0,
    "net_lines": 0
}}

RESUME:
{resume_text}

JOB DESCRIPTION:
{job_description}
"""

    try:
        # Increase timeout for complex tailoring operations
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )
        return response.candidates[0].content.parts[0].text
    except Exception as e:
        print(f"Error during AI tailoring: {e}")
        # Return empty replacements on error
        return json.dumps({"replacements": [], "lines_added": 0, "lines_freed": 0, "net_lines": 0})

def iterative_tailor_with_verification(docs_service, drive_service, copied_doc_id, original_doc_id,
                                        original_resume_text, original_resume_text_plain,
                                        job_description, line_metadata, line_budget, keywords,
                                        mimikree_data=None, max_iterations=3):
    """Iteratively tailor resume with verification and rollback capability.
    
    This function will:
    1. Apply tailoring changes
    2. Verify document length
    3. If overflow detected, restore from backup and retry with stricter constraints
    4. Repeat until success or max iterations reached
    
    Returns:
        Dictionary with:
        - success: True if tailoring succeeded without overflow
        - iterations: Number of iterations performed
        - final_length: Final document length in visual lines
        - final_doc_id: Final document ID (may change during rollback)
    """
    original_visual_lines = line_budget['current_visual_lines']
    
    # Get ACTUAL page count from PDF export (the ONLY reliable method)
    print(f"\nüìÑ Getting actual page count from original document...")
    original_page_count = get_actual_page_count(drive_service, original_doc_id)
    
    if original_page_count is None:
        print(f"‚ö†Ô∏è  Could not get PDF page count. Please install PyPDF2: pip install PyPDF2")
        # Fallback to rough estimate
        original_page_count = original_visual_lines / 40.0
        print(f"‚ö†Ô∏è  Using rough estimate: {original_page_count:.1f} pages")
    else:
        print(f"‚úÖ Original document: {original_page_count} page(s)")
    
    # Create a backup by copying the document again (for rollback)
    print(f"\nüîÑ Starting iterative tailoring (max {max_iterations} iterations)...")
    print(f"üìè ORIGINAL DOCUMENT BASELINE:")
    print(f"   Actual Pages: {original_page_count} (STRICT LIMIT - must not exceed)")
    print(f"   Visual lines estimate: {original_visual_lines}")
    print(f"   Target: Keep ‚â§ {original_page_count} page(s)")
    
    previous_validation_report = None
    
    for iteration in range(1, max_iterations + 1):
        print(f"\n{'='*60}")
        print(f"ITERATION {iteration}/{max_iterations}")
        print(f"{'='*60}")
        
        # Trust Gemini completely - don't artificially reduce on retries
        # Just regenerate with the same settings
        print(f"‚öôÔ∏è  Iteration {iteration}: Trusting Gemini's judgment (no artificial limits)")
        
        # Get tailoring suggestions
        print(f"\nüìù Getting tailoring suggestions (iteration {iteration})...")
        annotated_resume_text = annotate_resume_with_metadata(original_resume_text, line_metadata)
        
        replacements_json = tailor_resume(
            annotated_resume_text,
            job_description,
            line_metadata=line_metadata,
            line_budget=line_budget,
            keywords=keywords,
            mimikree_data=mimikree_data,
            previous_validation=previous_validation_report,
            iteration=iteration
        )
        
        # Before applying, create a backup of current state
        print(f"üíæ Creating backup before applying changes...")
        backup_doc_id = copy_google_doc(
            drive_service,
            copied_doc_id,
            f"BACKUP_ITER{iteration}_{uuid.uuid4().hex[:8]}"
        )
        
        # Apply changes
        print(f"üîß Applying tailoring changes...")
        apply_json_replacements_to_doc(
            docs_service,
            copied_doc_id,
            replacements_json,
            original_resume_text,
            job_description,
            line_metadata=line_metadata
        )
        
        # Verify document length using ACTUAL page count
        print(f"\nüìè Verifying document length (exporting to PDF to get actual pages)...")
        time.sleep(2)  # Wait for changes to settle
        verification = verify_document_length(docs_service, drive_service, copied_doc_id, original_page_count, tolerance=0)
        
        print(f"üìä PAGE COUNT VERIFICATION:")
        print(f"   Original Pages:  {verification['original_pages']}")
        print(f"   Current Pages:   {verification['current_pages']}")
        print(f"   Max Allowed:     {verification['max_allowed_pages']}")
        
        if verification['overflow_pages'] > 0:
            print(f"   ‚ö†Ô∏è  OVERFLOW: +{verification['overflow_pages']} page(s) over limit!")
        
        if verification['within_limit']:
            print(f"‚úÖ SUCCESS! Document is within {original_page_count} page limit.")
            
            # COMPREHENSIVE QUALITY VALIDATION
            print(f"\n{'='*60}")
            print(f"FINAL QUALITY VALIDATION (Iteration {iteration})")
            print(f"{'='*60}")
            
            # Read the tailored document
            tailored_doc = docs_service.documents().get(documentId=copied_doc_id).execute()
            tailored_content = tailored_doc.get('body', {}).get('content', [])
            tailored_text_plain = read_structural_elements_plain(tailored_content)
            
            # Extract job keywords
            job_keywords = keywords.get('prioritized_keywords', [])[:10] if keywords else []
            
            # Run comprehensive validation
            validator = MultiStageValidator()
            
            # Update line metadata with current text
            current_line_metadata = extract_document_structure(docs_service, copied_doc_id)
            
            validation_report = validator.validate_resume(
                resume_text=tailored_text_plain,
                line_metadata=current_line_metadata,
                job_keywords=job_keywords
            )
            
            # Store for next iteration
            previous_validation_report = validation_report
            
            # Check if resume meets quality standards
            if validation_report['is_ready']:
                print(f"\nüéâ QUALITY VALIDATION PASSED!")
                print(f"   Overall Score: {validation_report['overall_score']:.1f}/100")
                print(f"   Grade: {validation_report['grade']}")
                
                # Clean up backup
                try:
                    drive_service.files().delete(fileId=backup_doc_id).execute()
                    print(f"üóëÔ∏è  Backup deleted")
                except:
                    pass
                
                return {
                    'success': True,
                    'iterations': iteration,
                    'final_pages': verification['current_pages'],
                    'overflow_pages': 0,
                    'final_doc_id': copied_doc_id,
                    'quality_score': validation_report['overall_score'],
                    'quality_grade': validation_report['grade'],
                    'validation_report': validation_report
                }
            else:
                # Quality issues found but length is OK
                print(f"\n‚ö†Ô∏è  QUALITY ISSUES DETECTED!")
                print(f"   Overall Score: {validation_report['overall_score']:.1f}/100 (needs ‚â•85)")
                
                # Show critical issues
                line_issues = validation_report['validations']['line_quality']['critical_issues']
                content_issues = validation_report['validations']['content_quality']['poor_bullets_count']
                ats_issues = len([i for i in validation_report['validations']['ats']['issues'] 
                                if i['severity'] == 'critical'])
                
                if line_issues > 0:
                    print(f"   ‚ùå Line Quality: {line_issues} critical issues (orphaned words, etc.)")
                if content_issues > 0:
                    print(f"   ‚ùå Content Quality: {content_issues} bullets need improvement")
                if ats_issues > 0:
                    print(f"   ‚ùå ATS Compatibility: {ats_issues} critical issues")
                
                # If we have more iterations, continue; otherwise return with warning
                if iteration < max_iterations:
                    print(f"\nüîÑ Retrying with quality improvements (iteration {iteration + 1}/{max_iterations})...")
                    # Don't return yet, continue to next iteration
                    continue
                else:
                    print(f"\n‚ö†Ô∏è  Max iterations reached. Returning resume with quality warnings.")
                    
                    # Clean up backup
                    try:
                        drive_service.files().delete(fileId=backup_doc_id).execute()
                        print(f"üóëÔ∏è  Backup deleted")
                    except:
                        pass
                    
                    return {
                        'success': True,  # Length is OK
                        'quality_warning': True,
                        'iterations': iteration,
                        'final_pages': verification['current_pages'],
                        'overflow_pages': 0,
                        'final_doc_id': copied_doc_id,
                        'quality_score': validation_report['overall_score'],
                        'quality_grade': validation_report['grade'],
                        'validation_report': validation_report
                    }
        else:
            # Overflow detected - rollback
            overflow = verification['overflow_pages']
            print(f"‚ùå PAGE OVERFLOW DETECTED! Document is {verification['current_pages']} pages")
            print(f"   (limit is {verification['original_pages']} page(s), overflow: +{overflow:.2f} pages)")
            
            if iteration < max_iterations:
                print(f"üîÑ Rolling back changes and retrying with stricter constraints...")
                
                # Simple approach: just use the original document again
                # Delete the overflowing copy and create a fresh one
                try:
                    drive_service.files().delete(fileId=copied_doc_id).execute()
                    print(f"üóëÔ∏è  Deleted overflowing document")
                    
                    # Create fresh copy from original
                    copied_doc_id = copy_google_doc(
                        drive_service,
                        original_doc_id,
                        f"Retry_{iteration+1}_{uuid.uuid4().hex[:6]}"
                    )
                    print(f"‚úÖ Rollback successful - created fresh copy")
                except Exception as e:
                    print(f"‚ö†Ô∏è  Rollback failed: {e}")
                    # If rollback fails, we can't continue reliably
                    return {
                        'success': False,
                        'iterations': iteration,
                        'final_pages': verification['current_pages'],
                        'overflow_pages': overflow,
                        'final_doc_id': copied_doc_id
                    }
                
                # Clean up backup
                try:
                    drive_service.files().delete(fileId=backup_doc_id).execute()
                except:
                    pass
            else:
                print(f"‚ùå Max iterations reached. Could not fit changes within page limit.")
                # Clean up backup
                try:
                    drive_service.files().delete(fileId=backup_doc_id).execute()
                except:
                    pass
                
                return {
                    'success': False,
                    'iterations': iteration,
                    'final_pages': verification['current_pages'],
                    'overflow_pages': overflow,
                    'final_doc_id': copied_doc_id
                }
    
    # Should not reach here, but just in case
    return {
        'success': False,
        'iterations': max_iterations,
        'final_pages': original_page_count,
        'overflow_pages': 0,
        'final_doc_id': copied_doc_id
    }

def tailor_resume_and_return_url(original_resume_url, job_description, job_title, company,
                                   credentials=None, mimikree_email=None, mimikree_password=None):
    """Tailor resume and return publicly accessible Google Doc URL

    Args:
        original_resume_url: URL to the original Google Doc resume
        job_description: Job description text
        job_title: Job title
        company: Company name
        credentials: Optional Google OAuth2 Credentials object for user-specific access
        mimikree_email: Optional Mimikree account email for profile integration
        mimikree_password: Optional Mimikree account password for profile integration
    """
    try:
        # Get Google Services (with user-specific credentials if provided)
        docs_service, drive_service = get_google_services(credentials)
        if not all([docs_service, drive_service]):
            raise ValueError("Failed to authenticate with Google services")

        # Get Doc ID from URL
        original_doc_id = get_doc_id_from_url(original_resume_url)
        if not original_doc_id:
            raise ValueError("Invalid Google Doc URL provided")

        # Get original document name
        try:
            original_doc_name = drive_service.files().get(fileId=original_doc_id, fields='name').execute().get('name')
        except Exception as error:
            print(f"Error fetching document name: {error}")
            original_doc_name = "Resume"

        # Create a copy of the document
        copied_doc_title = f"{original_doc_name} - Tailored for {job_title} at {company}"
        print(f"Creating a copy of the document: '{copied_doc_title}'")
        copied_doc_id = copy_google_doc(drive_service, original_doc_id, copied_doc_title)
        if not copied_doc_id:
            raise ValueError("Could not copy the Google Doc")

        # Read content from the ORIGINAL document (optimize by getting document once)
        print("Reading original resume from Google Docs...")
        try:
            document = docs_service.documents().get(documentId=original_doc_id).execute()
        except Exception as e:
            raise ValueError(f"Could not read the Google Doc: {e}")

        # Get both styled and plain text content from the same document
        content = document.get('body', {}).get('content', [])
        original_resume_text = read_structural_elements(content)
        if not original_resume_text:
            raise ValueError("Could not read content from the original Google Doc")

        original_resume_text_plain = read_structural_elements_plain(content)

        # Check if enhanced tailoring is enabled (can be disabled for faster processing)
        use_enhanced_tailoring = os.getenv('USE_ENHANCED_TAILORING', 'true').lower() in ('1', 'true', 'yes')

        if use_enhanced_tailoring:
            print("Using enhanced tailoring with format preservation...")

            # Extract document structure and metadata
            print("Extracting document structure and formatting metadata...")
            try:
                line_metadata = extract_document_structure(docs_service, original_doc_id)
                print(f"Extracted metadata for {len(line_metadata)} lines")

                # Save metadata to file for debugging
                metadata_debug_file = f"../Resumes/metadata_debug_{uuid.uuid4().hex[:8]}.json"
                with open(metadata_debug_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'total_lines': len(line_metadata),
                        'job_title': job_title,
                        'company': company,
                        'lines': line_metadata
                    }, f, indent=2)
                print(f"üìÑ Metadata saved to: {metadata_debug_file}")

            except Exception as e:
                print(f"Warning: Structure extraction failed ({e}), using basic tailoring")
                use_enhanced_tailoring = False
                line_metadata = None

            if use_enhanced_tailoring and line_metadata:
                # Calculate line budget (0 additional lines = strict page preservation)
                line_budget = calculate_line_budget(line_metadata, max_additional_lines=0)
                print(f"Line budget: {line_budget['current_paragraphs']} paragraphs, {line_budget['current_visual_lines']} visual lines")
                print(f"‚ö†Ô∏è  STRICT MODE: Resume length must stay exactly the same!")

                # Extract keywords from job description (with timeout protection)
                print("Extracting keywords from job description...")
                try:
                    keywords = extract_job_keywords(job_description)
                    print(f"Extracted {len(keywords.get('prioritized_keywords', []))} prioritized keywords")
                    if keywords.get('key_themes'):
                        print(f"Key themes: {', '.join(keywords.get('key_themes', [])[:5])}")
                except Exception as e:
                    print(f"Warning: Keyword extraction failed, continuing without: {e}")
                    keywords = None

                # Integrate Mimikree profile data if credentials provided
                mimikree_data = None
                if mimikree_email and mimikree_password:
                    print("\nüîó Mimikree Integration Starting...")
                    print("="*60)

                    # Step 1: Authenticate with Mimikree
                    print("Step 1: Authenticating with Mimikree...")
                    auth_result = authenticate_mimikree(mimikree_email, mimikree_password)

                    if auth_result and auth_result.get('success'):
                        username = auth_result.get('username')
                        print(f"‚úÖ Authenticated as: {username}")

                        # Step 2: Generate questions based on job description
                        print("\nStep 2: Generating personalized questions...")
                        questions = generate_mimikree_questions(job_description, job_title, company)
                        print(f"‚úÖ Generated {len(questions)} questions")

                        # Step 3: Ask Mimikree chatbot the questions
                        print("\nStep 3: Querying Mimikree chatbot...")
                        responses = ask_mimikree_batch_questions(username, questions)

                        if responses and responses.get('success'):
                            print(f"‚úÖ Received {responses.get('successfulResponses')} responses")

                            # Step 4: Format responses for resume tailoring
                            print("\nStep 4: Formatting profile data for resume tailoring...")
                            mimikree_data = format_mimikree_responses_for_resume(responses)

                            # Save Mimikree data for debugging
                            mimikree_debug_file = f"../Resumes/mimikree_data_{uuid.uuid4().hex[:8]}.txt"
                            with open(mimikree_debug_file, 'w', encoding='utf-8') as f:
                                f.write(mimikree_data)
                            print(f"üìÑ Mimikree data saved to: {mimikree_debug_file}")
                            print(f"‚úÖ Mimikree integration complete!")
                        else:
                            print("‚ö†Ô∏è Failed to get responses from Mimikree chatbot")
                    else:
                        print("‚ö†Ô∏è Mimikree authentication failed - proceeding without profile data")

                    print("="*60)
                else:
                    print("\n‚ö†Ô∏è Mimikree credentials not provided - skipping profile integration")
                    print("   To use Mimikree integration, provide mimikree_email and mimikree_password")

                # Annotate resume with metadata for AI guidance
                print("\nAnnotating resume with formatting constraints...")
                annotated_resume_text = annotate_resume_with_metadata(original_resume_text, line_metadata)

                # Save annotated resume for debugging
                annotated_debug_file = f"../Resumes/annotated_resume_{uuid.uuid4().hex[:8]}.txt"
                with open(annotated_debug_file, 'w', encoding='utf-8') as f:
                    f.write("=== ANNOTATED RESUME (What Gemini Receives) ===\n\n")
                    f.write(annotated_resume_text)
                print(f"üìÑ Annotated resume saved to: {annotated_debug_file}")
            else:
                line_budget = None
                keywords = None
                mimikree_data = None
                annotated_resume_text = original_resume_text
        else:
            print("Using basic tailoring (set USE_ENHANCED_TAILORING=true for format preservation)...")
            line_metadata = None
            line_budget = None
            keywords = None
            mimikree_data = None
            annotated_resume_text = original_resume_text

        # Use iterative tailoring with verification and rollback
        print("üöÄ Starting iterative tailoring with automatic verification...")
        result = iterative_tailor_with_verification(
            docs_service,
            drive_service,
            copied_doc_id,
            original_doc_id,
            original_resume_text,
            original_resume_text_plain,
            job_description,
            line_metadata,
            line_budget,
            keywords,
            mimikree_data=mimikree_data,
            max_iterations=3
        )
        
        # Report results
        if result['success']:
            print(f"\n{'='*60}")
            print(f"‚úÖ TAILORING SUCCESSFUL!")
            print(f"   Completed in {result['iterations']} iteration(s)")
            print(f"   Final page count: {result['final_pages']} page(s)")
            print(f"   Status: Within page limit ‚úì")
            
            # Show quality score if available
            if 'quality_score' in result:
                print(f"\nüìä QUALITY METRICS:")
                print(f"   Overall Score: {result['quality_score']:.1f}/100")
                print(f"   Grade: {result['quality_grade']}")
                
                if 'validation_report' in result:
                    scores = result['validation_report']['scores']
                    print(f"   ‚Ä¢ Line Quality: {scores['line_quality']:.1f}/100")
                    print(f"   ‚Ä¢ Content Quality: {scores['content_quality']:.1f}/100")
                    print(f"   ‚Ä¢ ATS Compatibility: {scores['ats']:.1f}/100")
                
                # Show if quality warning
                if result.get('quality_warning'):
                    print(f"\n‚ö†Ô∏è  Some quality issues remain - review validation report")
            
            print(f"{'='*60}\n")
        else:
            print(f"\n{'='*60}")
            print(f"‚ö†Ô∏è  TAILORING COMPLETED WITH WARNINGS")
            print(f"   Iterations performed: {result['iterations']}")
            print(f"   Final page count: {result['final_pages']} page(s)")
            print(f"   Overflow: {result['overflow_pages']:.2f} page(s)")
            print(f"   Status: Exceeded page limit by {result['overflow_pages']:.2f} pages")
            print(f"{'='*60}\n")
            print(f"‚ö†Ô∏è  Document exceeded page limit. Manual review required.")
        
        # Update copied_doc_id in case it changed during rollback
        copied_doc_id = result.get('final_doc_id', copied_doc_id)

        # Make the document publicly accessible
        print("Making document publicly accessible...")
        permission = {
            'role': 'reader',
            'type': 'anyone'
        }
        
        drive_service.permissions().create(
            fileId=copied_doc_id,
            body=permission
        ).execute()

        # Also download the final resume as a PDF (for backup)
        print("Downloading the tailored resume as a PDF...")
        output_pdf_path = f"../Resumes/{copied_doc_title}.pdf"
        download_doc_as_pdf(drive_service, copied_doc_id, output_pdf_path)

        # Return the public URL
        tailored_doc_url = f"https://docs.google.com/document/d/{copied_doc_id}/edit"
        print(f"‚úÖ Tailored resume created and made public: {tailored_doc_url}")
        return tailored_doc_url

    except Exception as e:
        print(f"Error in tailor_resume_and_return_url: {e}")
        raise
    
if __name__ == "__main__":
    # Example usage - replace with your actual values
    google_doc_url = "https://docs.google.com/document/d/1flfyzOJ_5sOklftoq76HLErYDmEYYdOsEsAB4G4ZMIs/edit?tab=t.0"

    # Optional: Mimikree credentials for profile integration
    # If not provided, the agent will still work but without personalized profile data
    mimikree_email = os.getenv("MIMIKREE_EMAIL", "schordi@umd.edu")  # Set in .env file or provide directly
    mimikree_password = os.getenv("MIMIKREE_PASSWORD", "Saahil@2412")  # Set in .env file or provide directly

    job_description = """
    Figma is growing our team of passionate creatives and builders on a mission to make design accessible to all. Figma's platform helps teams bring ideas to life‚Äîwhether you're brainstorming, creating a prototype, translating designs into code, or iterating with AI. From idea to product, Figma empowers teams to streamline workflows, move faster, and work together in real time from anywhere in the world. If you're excited to shape the future of design and collaboration, join us!

We‚Äôre looking for Data Science Interns who are excited to use data to answer big questions and guide decisions across Figma. At Figma, interns are embedded into small, collaborative teams where they‚Äôll partner closely with engineers, PMs, and designers to make sense of data, build models, and surface insights that shape our product and business.

This internship will be based out of our San Francisco or New York hub.

What you‚Äôll do at Figma:
Collaborate across teams to turn business questions into data problems
Design experiments and evaluate metrics to guide product decisions
Build models or conduct analyzes that help us understand behavior and growth
Develop tools, datasets, or dashboards that make data more accessible to others
Share insights with both technical and non-technical teammates
Some projects you could work on:

Investigate user behavior and recommend product improvements
Improve experimentation accuracy and velocity through new testing methodologies
Build internal datasets and models to support product, marketing, or business teams
Explore growth opportunities through critical metric analysis and funnel research
We‚Äôd love to hear from you if you have:

Investigate user behavior and recommend product improvements
Improve experimentation accuracy and velocity through new testing methodologies
Build internal datasets and models to support product, marketing, or business teams
Explore growth opportunities through critical metric analysis and funnel research
At Figma, one of our values is Grow as you go. We believe in hiring smart, curious people who are excited to learn and develop their skills. If you‚Äôre excited about this role but your past experience doesn‚Äôt align perfectly with the points outlined in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.

#LI-HO1
Pay Transparency Disclosure

This internship role is based in either Figma‚Äôs San Francisco or New York hub offices, and has the hourly base pay rate stated below.  Figma also offers interns a housing stipend and travel reimbursement. Figma‚Äôs compensation and benefits are subject to change and may be modified in the future.

Internship

$44.71 - $44.71 USD

At Figma we celebrate and support our differences. We know employing a team rich in diverse thoughts, experiences, and opinions allows our employees, our product and our community to flourish. Figma is an equal opportunity workplace - we are dedicated to equal employment opportunities regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity/expression, veteran status, or any other characteristic protected by law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.

We will work to ensure individuals with disabilities are provided reasonable accommodation to apply for a role, participate in the interview process, perform essential job functions, and receive other benefits and privileges of employment. If you require accommodation, please reach out to accommodations-ext@figma.com. These modifications enable an individual with a disability to have an equal opportunity not only to get a job, but successfully perform their job tasks to the same extent as people without disabilities. 

Examples of accommodations include but are not limited to: 

Holding interviews in an accessible location
Enabling closed captioning on video conferencing
Ensuring all written communication be compatible with screen readers
Changing the mode or format of interviews 
To ensure the integrity of our hiring process and facilitate a more personal connection, we require all candidates keep their cameras on during video interviews. Additionally, if hired you will be required to attend in person onboarding.

By applying for this job, the candidate acknowledges and agrees that any personal data contained in their application or supporting materials will be processed in accordance with Figma's Candidate Privacy Notice.
    """
    job_title = "Data Scientist"
    company = "Figma"

    try:
        # Call with Mimikree integration
        tailored_url = tailor_resume_and_return_url(
            google_doc_url,
            job_description,
            job_title,
            company,
            mimikree_email=mimikree_email,
            mimikree_password=mimikree_password
        )
        print(f"Tailored resume created: {tailored_url}")
    except Exception as e:
        print(f"Error: {e}")

